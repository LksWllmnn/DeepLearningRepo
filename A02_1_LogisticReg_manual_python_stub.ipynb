{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TmURzPPRtODn"},"source":["# Logistic Regression Assignment (due 26 November)\n","\n","In this practical you will learn how to apply logistic regression to the task of predicting two digits from the MNIST database: http://yann.lecun.com/exdb/mnist/. The database contains 60000 train images containing digits and 10000 test images. The images are of size 28 Ã— 28. We will use the images in a vectorized form: a vector of size of 784. The code extracting the digits 0 and 1 is provided in the stubs.\n"]},{"cell_type":"code","metadata":{"id":"MQkzOf0dFpxc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e294ee2b-239b-4ea3-c89c-06fb64509145","executionInfo":{"status":"ok","timestamp":1682000610239,"user_tz":-120,"elapsed":3,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["%tensorflow_version 2.x"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}]},{"cell_type":"code","metadata":{"id":"xI0dT0-WEYuh","executionInfo":{"status":"ok","timestamp":1682000624137,"user_tz":-120,"elapsed":11539,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqiqzu3dFXKj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fc756cf-1079-4bf2-cd3f-c5f19163adc8","executionInfo":{"status":"ok","timestamp":1682000626956,"user_tz":-120,"elapsed":226,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["print(tf.__version__)\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"ZSjE8NINEvVO","executionInfo":{"status":"ok","timestamp":1682000630084,"user_tz":-120,"elapsed":203,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["import numpy as np "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybQxEwHnGsgY","executionInfo":{"status":"ok","timestamp":1682000744121,"user_tz":-120,"elapsed":194,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["def data_preprocess(images, labels):\n","\n","    # number of examples m  \n","    m = images.shape[0]\n","    \n","    print(m)\n","    # create vector of ones to concatenate to our data matrix (for intercept terms)\n","    ones = np.ones(shape=[m, 1])\n","    images = np.concatenate((ones, images), axis=1)\n","    \n","    # to retrieve the images and corresponding labels where the label is either 0 or 1, \n","    # we define two logical vectors that can be used to subset our data_matrices\n","    logical_mask_0 = labels == 0\n","    logical_mask_1 = labels == 1\n","    \n","    images_zeros = images[logical_mask_0]\n","    labels_zeros = labels[logical_mask_0]\n","    images_ones = images[logical_mask_1]\n","    labels_ones = labels[logical_mask_1]\n","    \n","    X = np.concatenate((images_zeros, images_ones), axis=0)\n","    y = np.concatenate((labels_zeros, labels_ones), axis=0)\n","    \n","    # shuffle the data and corresponding labels in unison\n","    def _shuffle_in_unison(a, b):\n","        assert len(a) == len(b)\n","        p = np.random.permutation(len(a))\n","        print('length ', len(a))\n","        print(a.shape)\n","        print(a[p].shape)\n","        return a[p], b[p]\n","\n","    return _shuffle_in_unison(X,y)   "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrkpG7BYI8MT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682000777777,"user_tz":-120,"elapsed":956,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}},"outputId":"e91e3df5-48f6-4eca-9c0f-b099cb2d2940"},"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"CHEiLlEnNhIm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bbf6425-8966-420d-c583-d1e86ca71c9c","executionInfo":{"status":"ok","timestamp":1682000934902,"user_tz":-120,"elapsed":226,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["print (x_train.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n"]}]},{"cell_type":"code","metadata":{"id":"Cb5Hr9aENMTy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8bcee5ae-0b15-493a-bac7-db976b2e265d","executionInfo":{"status":"ok","timestamp":1682001052931,"user_tz":-120,"elapsed":212,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["x_train = x_train.reshape([60000,784])\n","x_test = x_test.reshape([10000,784])\n","print(x_train.shape)\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n"]}]},{"cell_type":"code","metadata":{"id":"cMi47AaKcHzQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3af5dd7-17f1-47f2-d001-e33c9b8c02c3","executionInfo":{"status":"ok","timestamp":1682001059957,"user_tz":-120,"elapsed":520,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["X,y = data_preprocess(x_train, y_train)\n","print('shape: ', X.shape)\n","print('shape: ', y.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","length  12665\n","(12665, 785)\n","(12665, 785)\n","shape:  (12665, 785)\n","shape:  (12665,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"UTx9l6kePGbf"},"source":["Define hyperparams: learning rate and gradient descent steps\n"]},{"cell_type":"code","metadata":{"id":"NzE8JqGXdBy2","executionInfo":{"status":"ok","timestamp":1682002772654,"user_tz":-120,"elapsed":191,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["learning_rate = 0.01\n","gdc_steps = 1000\n","\n","    "],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5ujHEHrQDy_"},"source":["Initialize your parameters W\n"]},{"cell_type":"code","metadata":{"id":"RnXlpZqMR4jP","executionInfo":{"status":"ok","timestamp":1682002774370,"user_tz":-120,"elapsed":273,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["# number of features n\n","n = X.shape[1]\n","# we need to define our model parameters to be learned. we use W (weights) instead of theta this time.\n","mu, sigma = 0, 0.01 # mean and standard deviation\n","w = np.random.normal(mu, sigma, n)\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"1b-fVYzzR6_r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed4482d2-d7e5-4437-bd77-ae4bc2eb2a67","executionInfo":{"status":"ok","timestamp":1682002776152,"user_tz":-120,"elapsed":196,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["print(X.shape, w.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["(12665, 785) (785,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Yp3Uc6qUSIi0"},"source":["Define the sigmoid function, your code here:\n"]},{"cell_type":"code","metadata":{"id":"O_gdiFglSMYN","executionInfo":{"status":"ok","timestamp":1682002777901,"user_tz":-120,"elapsed":200,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m14UBJJ6SRGS"},"source":["Define the loss function as provided in equation 12 (Logistic regression slides)\n"]},{"cell_type":"code","metadata":{"id":"NyCliB0USThU","executionInfo":{"status":"ok","timestamp":1682002780033,"user_tz":-120,"elapsed":205,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["def compute_cross_entropy_loss(y, y_hat):\n","    n = len(y)\n","    loss = 0\n","    for i in range(n):\n","        loss += -y[i] * np.log(y_hat[i]) - (1 - y[i]) * np.log(1 - y_hat[i])\n","    return loss / n\n","\n"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vusNro8SX5-"},"source":["Start optimization. During training you minimize the loss function. In every iteration your loss should decrease. You also want to look how many correct predictions you have at every iteration. Reminder: the belonging to class digit 1 is when your prediction, $\\hat y$ is greater or equal to 0.5. "]},{"cell_type":"markdown","metadata":{"id":"fKXM0YdDcajm"},"source":["When you test your prediction vector (containing zero and ones) with the labels (also zero and ones) you can use the equal function. \n","\n","Example:\n","prediction = (1, 0, 1, 1) and the true labels are y = (0, 0, 1, 0).\n","\n","When you test on equality you get following result: correct = (0, 1, 1, 0). Your accuracy is: 0+1+1+0\n","4 = 0.5.\n","You compute the accuracy for the training and test."]},{"cell_type":"code","metadata":{"id":"5ToLLOp2Scv0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682003035943,"user_tz":-120,"elapsed":253979,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}},"outputId":"b5d677d1-9d26-409f-e531-d7d6be1e74a2"},"source":["for step in range(0, gdc_steps):\n","    print(\"Performing step \" + str(step) + \" of gradient descent.\")\n","    # perform the dot product between the weights and the examples\n","    z = np.dot(X,w)\n","    print('z', z, z.shape)\n","    # apply the nonlinearity\n","    y_hat = sigmoid(z)\n","    print(\"y hat: \" + str(y_hat))\n","    # normally normalized with -1/m \n","    loss = compute_cross_entropy_loss(y, y_hat)\n","    print(\"Loss at step \" + str(step) + \": \" + str(loss))\n","    \n","    # compute the error term, i.e. the difference between labels and estimated labels y_hat, see equation 24 in the slides\n","    error_term = y_hat - y\n","    \n","    # compute the gradient. as our data matrix X is currently layed out as X_j_i, we got to transpose it\n","    # see derived formula of the gradient calculation\n","    gradients = (1/len(y)) * np.dot(X.T, error_term)\n","    print(X.shape)\n","    print(error_term.shape)\n","    print(gradients.shape)\n","    \n","    # update w using the gdc update rule\n","    w = w-learning_rate*gradients\n","    \n","    # compute the predictions and cast them to int values\n","    predictions = np.int64(y_hat >= 0.5)\n","    print(predictions.shape)\n","    # compute mean accuracy\n","    accuracy = np.mean(predictions == y)\n","    print(\"Accuracy at step \" + str(step) + \": \" + str(accuracy))\n","    \n"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mDie letzten 5000Â Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n","Performing step 500 of gradient descent.\n","z [ 4.41206362  3.12863191  2.02957809 ... -5.42279563  1.14367104\n","  2.07914355] (12665,)\n","y hat: [0.98801526 0.95805845 0.88386778 ... 0.00439538 0.75835301 0.88885945]\n","Loss at step 500: -5.3501933442619105\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 500: 0.9971575207264114\n","Performing step 501 of gradient descent.\n","z [ 4.41465065  3.13029319  2.03090646 ... -5.42625219  1.14456346\n","  2.08055871] (12665,)\n","y hat: [0.98804585 0.95812516 0.88400406 ... 0.00438028 0.75851651 0.88899918]\n","Loss at step 501: -5.342353046421459\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 501: 0.9971575207264114\n","Performing step 502 of gradient descent.\n","z [ 4.41723273  3.13195107  2.03223249 ... -5.4297027   1.14545445\n","  2.08197153] (12665,)\n","y hat: [0.98807631 0.95819162 0.88413996 ... 0.00436526 0.75867967 0.88913852]\n","Loss at step 502: -5.334541622757153\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 502: 0.9971575207264114\n","Performing step 503 of gradient descent.\n","z [ 4.41980987  3.13360556  2.0335562  ... -5.43314717  1.146344\n","  2.08338203] (12665,)\n","y hat: [0.98810663 0.95825785 0.88427549 ... 0.00435032 0.7588425  0.88927748]\n","Loss at step 503: -5.326758908998185\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 503: 0.9971575207264114\n","Performing step 504 of gradient descent.\n","z [ 4.42238211  3.13525668  2.03487759 ... -5.43658564  1.14723213\n","  2.08479022] (12665,)\n","y hat: [0.98813682 0.95832385 0.88441064 ... 0.00433545 0.75900499 0.88941606]\n","Loss at step 504: -5.319004742132178\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 504: 0.9971575207264114\n","Performing step 505 of gradient descent.\n","z [ 4.42494944  3.13690443  2.03619666 ... -5.44001812  1.14811885\n","  2.08619611] (12665,)\n","y hat: [0.98816688 0.95838961 0.88454542 ... 0.00432066 0.75916715 0.88955426]\n","Loss at step 505: -5.311278960393561\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 505: 0.9971575207264114\n","Performing step 506 of gradient descent.\n","z [ 4.4275119   3.13854884  2.03751344 ... -5.44344463  1.14900415\n","  2.0875997 ] (12665,)\n","y hat: [0.98819681 0.95845514 0.88467983 ... 0.00430594 0.75932897 0.88969208]\n","Loss at step 506: -5.303581403251057\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 506: 0.9971575207264114\n","Performing step 507 of gradient descent.\n","z [ 4.43006951  3.14018991  2.03882793 ... -5.44686519  1.14988804\n","  2.089001  ] (12665,)\n","y hat: [0.9882266  0.95852043 0.88481387 ... 0.0042913  0.75949047 0.88982953]\n","Loss at step 507: -5.295911911396298\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 507: 0.9971575207264114\n","Performing step 508 of gradient descent.\n","z [ 4.43262227  3.14182766  2.04014013 ... -5.45027983  1.15077053\n","  2.09040003] (12665,)\n","y hat: [0.98825627 0.9585855  0.88494754 ... 0.00427673 0.75965163 0.8899666 ]\n","Loss at step 508: -5.288270326731962\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 508: 0.9971575207264114\n","Performing step 509 of gradient descent.\n","z [ 4.43517022  3.1434621   2.04145006 ... -5.45368857  1.15165163\n","  2.09179678] (12665,)\n","y hat: [0.9882858  0.95865034 0.88508084 ... 0.00426224 0.75981246 0.89010331]\n","Loss at step 509: -5.280656492360196\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 509: 0.9971575207264114\n","Performing step 510 of gradient descent.\n","z [ 4.43771337  3.14509325  2.04275772 ... -5.45709143  1.15253133\n","  2.09319128] (12665,)\n","y hat: [0.98831521 0.95871495 0.88521378 ... 0.00424782 0.75997297 0.89023964]\n","Loss at step 510: -5.273070252571234\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 510: 0.9971575207264114\n","Performing step 511 of gradient descent.\n","z [ 4.44025174  3.14672112  2.04406312 ... -5.46048842  1.15340965\n","  2.09458353] (12665,)\n","y hat: [0.98834448 0.95877933 0.88534635 ... 0.00423348 0.76013315 0.89037561]\n","Loss at step 511: -5.265511452832061\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 511: 0.9971575207264114\n","Performing step 512 of gradient descent.\n","z [ 4.44278534  3.14834571  2.04536627 ... -5.46387958  1.15428659\n","  2.09597353] (12665,)\n","y hat: [0.98837363 0.95884349 0.88547857 ... 0.00421921 0.76029301 0.89051121]\n","Loss at step 512: -5.257979939775387\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 512: 0.9971575207264114\n","Performing step 513 of gradient descent.\n","z [ 4.44531419  3.14996705  2.04666719 ... -5.46726491  1.15516215\n","  2.09736129] (12665,)\n","y hat: [0.98840266 0.95890742 0.88561042 ... 0.00420501 0.76045254 0.89064645]\n","Loss at step 513: -5.250475561188419\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 513: 0.9971575207264114\n","Performing step 514 of gradient descent.\n","z [ 4.44783833  3.15158515  2.04796587 ... -5.47064445  1.15603635\n","  2.09874683] (12665,)\n","y hat: [0.98843156 0.95897114 0.88574192 ... 0.00419088 0.76061175 0.89078132]\n","Loss at step 514: -5.2429981660021\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 514: 0.9971575207264114\n","Performing step 515 of gradient descent.\n","z [ 4.45035775  3.15320001  2.04926233 ... -5.47401821  1.15690918\n","  2.10013015] (12665,)\n","y hat: [0.98846033 0.95903463 0.88587306 ... 0.00417683 0.76077064 0.89091583]\n","Loss at step 515: -5.235547604280281\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 515: 0.9971575207264114\n","Performing step 516 of gradient descent.\n","z [ 4.45287248  3.15481166  2.05055657 ... -5.47738621  1.15778065\n","  2.10151126] (12665,)\n","y hat: [0.98848898 0.9590979  0.88600384 ... 0.00416284 0.76092921 0.89104998]\n","Loss at step 516: -5.228123727209094\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 516: 0.9971575207264114\n","Performing step 517 of gradient descent.\n","z [ 4.45538253  3.1564201   2.0518486  ... -5.48074847  1.15865077\n","  2.10289016] (12665,)\n","y hat: [0.9885175  0.95916095 0.88613428 ... 0.00414892 0.76108747 0.89118377]\n","Loss at step 517: -5.220726387086205\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 517: 0.9971575207264114\n","Performing step 518 of gradient descent.\n","z [ 4.45788794  3.15802534  2.05313843 ... -5.48410502  1.15951954\n","  2.10426688] (12665,)\n","y hat: [0.98854591 0.95922378 0.88626436 ... 0.00413508 0.7612454  0.89131721]\n","Loss at step 518: -5.213355437310724\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 518: 0.9971575207264114\n","Performing step 519 of gradient descent.\n","z [ 4.4603887   3.15962741  2.05442608 ... -5.48745587  1.16038697\n","  2.1056414 ] (12665,)\n","y hat: [0.98857419 0.9592864  0.88639409 ... 0.0041213  0.76140302 0.89145029]\n","Loss at step 519: -5.206010732372612\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 519: 0.9971575207264114\n","Performing step 520 of gradient descent.\n","z [ 4.46288485  3.1612263   2.05571154 ... -5.49080104  1.16125306\n","  2.10701375] (12665,)\n","y hat: [0.98860235 0.9593488  0.88652347 ... 0.0041076  0.76156033 0.89158301]\n","Loss at step 520: -5.19869212784267\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 520: 0.9971575207264114\n","Performing step 521 of gradient descent.\n","z [ 4.46537639  3.16282204  2.05699482 ... -5.49414055  1.16211782\n","  2.10838392] (12665,)\n","y hat: [0.98863039 0.95941098 0.8866525  ... 0.00409396 0.76171732 0.89171539]\n","Loss at step 521: -5.1913994803624215\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 521: 0.9971575207264114\n","Performing step 522 of gradient descent.\n","z [ 4.46786334  3.16441464  2.05827594 ... -5.49747443  1.16298125\n","  2.10975193] (12665,)\n","y hat: [0.98865831 0.95947296 0.88678119 ... 0.00408039 0.761874   0.89184741]\n","Loss at step 522: -5.184132647633992\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 522: 0.9971575207264114\n","Performing step 523 of gradient descent.\n","z [ 4.47034573  3.1660041   2.05955489 ... -5.50080269  1.16384335\n","  2.11111779] (12665,)\n","y hat: [0.98868611 0.95953472 0.88690953 ... 0.00406689 0.76203037 0.89197908]\n","Loss at step 523: -5.176891488410613\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 523: 0.9971575207264114\n","Performing step 524 of gradient descent.\n","z [ 4.47282357  3.16759045  2.0608317  ... -5.50412535  1.16470414\n","  2.1124815 ] (12665,)\n","y hat: [0.98871379 0.95959627 0.88703753 ... 0.00405345 0.76218643 0.89211041]\n","Loss at step 524: -5.169675862486414\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 524: 0.9971575207264114\n","Performing step 525 of gradient descent.\n","z [ 4.47529688  3.16917368  2.06210636 ... -5.50744243  1.16556362\n","  2.11384308] (12665,)\n","y hat: [0.98874136 0.95965761 0.8871652  ... 0.00404008 0.76234218 0.89224139]\n","Loss at step 525: -5.162485630687255\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 525: 0.9971575207264114\n","Performing step 526 of gradient descent.\n","z [ 4.47776566  3.17075382  2.06337889 ... -5.51075395  1.16642179\n","  2.11520252] (12665,)\n","y hat: [0.98876881 0.95971874 0.88729252 ... 0.00402678 0.76249763 0.89237203]\n","Loss at step 526: -5.155320654860794\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 526: 0.9971575207264114\n","Performing step 527 of gradient descent.\n","z [ 4.48022995  3.17233088  2.06464929 ... -5.51405993  1.16727865\n","  2.11655984] (12665,)\n","y hat: [0.98879614 0.95977966 0.8874195  ... 0.00401354 0.76265277 0.89250232]\n","Loss at step 527: -5.1481807978674095\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 527: 0.9971575207264114\n","Performing step 528 of gradient descent.\n","z [ 4.48268976  3.17390486  2.06591757 ... -5.51736039  1.16813422\n","  2.11791504] (12665,)\n","y hat: [0.98882336 0.95984038 0.88754615 ... 0.00400037 0.7628076  0.89263227]\n","Loss at step 528: -5.1410659235706575\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 528: 0.9971575207264114\n","Performing step 529 of gradient descent.\n","z [ 4.48514509  3.17547578  2.06718374 ... -5.52065535  1.16898849\n","  2.11926813] (12665,)\n","y hat: [0.98885046 0.95990089 0.88767246 ... 0.00398726 0.76296213 0.89276188]\n","Loss at step 529: -5.1339758968281055\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 529: 0.9971575207264114\n","Performing step 530 of gradient descent.\n","z [ 4.48759598  3.17704365  2.0684478  ... -5.52394483  1.16984148\n","  2.12061912] (12665,)\n","y hat: [0.98887745 0.95996119 0.88779844 ... 0.00397422 0.76311636 0.89289115]\n","Loss at step 530: -5.126910583482323\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 530: 0.9971575207264114\n","Performing step 531 of gradient descent.\n","z [ 4.49004244  3.17860849  2.06970976 ... -5.52722884  1.17069318\n","  2.12196802] (12665,)\n","y hat: [0.98890433 0.96002129 0.88792408 ... 0.00396124 0.76327029 0.89302009]\n","Loss at step 531: -5.119869850351722\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 531: 0.9971575207264114\n","Performing step 532 of gradient descent.\n","z [ 4.49248448  3.1801703   2.07096964 ... -5.5305074   1.17154361\n","  2.12331484] (12665,)\n","y hat: [0.98893109 0.96008119 0.8880494  ... 0.00394833 0.76342392 0.89314869]\n","Loss at step 532: -5.112853565221777\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 532: 0.9971575207264114\n","Performing step 533 of gradient descent.\n","z [ 4.49492211  3.18172909  2.07222743 ... -5.53378054  1.17239276\n","  2.12465957] (12665,)\n","y hat: [0.98895774 0.96014089 0.88817438 ... 0.00393547 0.76357725 0.89327696]\n","Loss at step 533: -5.105861596836125\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 533: 0.9971575207264114\n","Performing step 534 of gradient descent.\n","z [ 4.49735537  3.18328488  2.07348315 ... -5.53704826  1.17324064\n","  2.12600223] (12665,)\n","y hat: [0.98898428 0.96020039 0.88829904 ... 0.00392269 0.76373028 0.89340489]\n","Loss at step 534: -5.098893814887839\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 534: 0.9971575207264114\n","Performing step 535 of gradient descent.\n","z [ 4.49978425  3.18483768  2.0747368  ... -5.5403106   1.17408726\n","  2.12734284] (12665,)\n","y hat: [0.98901071 0.96025969 0.88842337 ... 0.00390996 0.76388301 0.89353249]\n","Loss at step 535: -5.091950090010893\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 535: 0.9971575207264114\n","Performing step 536 of gradient descent.\n","z [ 4.50220878  3.1863875   2.07598839 ... -5.54356757  1.17493261\n","  2.12868138] (12665,)\n","y hat: [0.98903703 0.96031879 0.88854738 ... 0.0038973  0.76403545 0.89365976]\n","Loss at step 536: -5.0850302937715925\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 536: 0.9971575207264114\n","Performing step 537 of gradient descent.\n","z [ 4.50462898  3.18793435  2.07723793 ... -5.54681918  1.17577672\n","  2.13001788] (12665,)\n","y hat: [0.98906324 0.96037769 0.88867106 ... 0.00388469 0.7641876  0.89378671]\n","Loss at step 537: -5.0781342986600935\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 537: 0.9971575207264114\n","Performing step 538 of gradient descent.\n","z [ 4.50704486  3.18947824  2.07848542 ... -5.55006546  1.17661957\n","  2.13135233] (12665,)\n","y hat: [0.98908935 0.9604364  0.88879442 ... 0.00387215 0.76433945 0.89391332]\n","Loss at step 538: -5.0712619780821\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 538: 0.9971575207264114\n","Performing step 539 of gradient descent.\n","z [ 4.50945643  3.19101918  2.07973088 ... -5.55330642  1.17746117\n","  2.13268475] (12665,)\n","y hat: [0.98911534 0.96049491 0.88891746 ... 0.00385967 0.76449101 0.89403961]\n","Loss at step 539: -5.064413206350694\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 539: 0.9971575207264114\n","Performing step 540 of gradient descent.\n","z [ 4.51186371  3.19255718  2.0809743  ... -5.55654208  1.17830154\n","  2.13401514] (12665,)\n","y hat: [0.98914123 0.96055323 0.88904018 ... 0.00384725 0.76464228 0.89416558]\n","Loss at step 540: -5.057587858677901\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 540: 0.9971575207264114\n","Performing step 541 of gradient descent.\n","z [ 4.51426672  3.19409226  2.0822157  ... -5.55977246  1.17914067\n","  2.13534351] (12665,)\n","y hat: [0.98916701 0.96061135 0.88916258 ... 0.00383489 0.76479326 0.89429122]\n","Loss at step 541: -5.050785811166992\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 541: 0.9971575207264114\n","Performing step 542 of gradient descent.\n","z [ 4.51666548  3.19562443  2.08345509 ... -5.56299758  1.17997856\n","  2.13666987] (12665,)\n","y hat: [0.98919268 0.96066928 0.88928467 ... 0.00382259 0.76494395 0.89441654]\n","Loss at step 542: -5.044006940804219\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 542: 0.9971575207264114\n","Performing step 543 of gradient descent.\n","z [ 4.51905999  3.19715369  2.08469247 ... -5.56621744  1.18081523\n","  2.13799423] (12665,)\n","y hat: [0.98921825 0.96072703 0.88940644 ... 0.00381035 0.76509435 0.89454154]\n","Loss at step 543: -5.037251125450969\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 543: 0.9971575207264114\n","Performing step 544 of gradient descent.\n","z [ 4.52145027  3.19868006  2.08592784 ... -5.56943209  1.18165068\n","  2.13931658] (12665,)\n","y hat: [0.98924371 0.96078458 0.8895279  ... 0.00379816 0.76524447 0.89466622]\n","Loss at step 544: -5.030518243836043\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 544: 0.9971575207264114\n","Performing step 545 of gradient descent.\n","z [ 4.52383634  3.20020355  2.08716122 ... -5.57264152  1.1824849\n","  2.14063694] (12665,)\n","y hat: [0.98926907 0.96084194 0.88964904 ... 0.00378604 0.7653943  0.89479059]\n","Loss at step 545: -5.023808175547809\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 545: 0.9971575207264114\n","Performing step 546 of gradient descent.\n","z [ 4.52621822  3.20172417  2.08839261 ... -5.57584575  1.18331791\n","  2.14195532] (12665,)\n","y hat: [0.98929433 0.96089911 0.88976987 ... 0.00377397 0.76554385 0.89491464]\n","Loss at step 546: -5.017120801026616\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 546: 0.9971575207264114\n","Performing step 547 of gradient descent.\n","z [ 4.52859591  3.20324192  2.08962202 ... -5.57904481  1.18414972\n","  2.14327172] (12665,)\n","y hat: [0.98931948 0.96095609 0.88989039 ... 0.00376196 0.76569311 0.89503837]\n","Loss at step 547: -5.010456001557199\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 547: 0.9971575207264114\n","Performing step 548 of gradient descent.\n","z [ 4.53096944  3.20475682  2.09084945 ... -5.58223872  1.18498031\n","  2.14458615] (12665,)\n","y hat: [0.98934453 0.96101289 0.89001061 ... 0.00375001 0.7658421  0.89516179]\n","Loss at step 548: -5.003813659261279\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 548: 0.9971575207264114\n","Performing step 549 of gradient descent.\n","z [ 4.53333881  3.20626889  2.09207492 ... -5.58542748  1.18580971\n","  2.14589861] (12665,)\n","y hat: [0.98936948 0.96106951 0.89013051 ... 0.00373812 0.7659908  0.8952849 ]\n","Loss at step 549: -4.997193657089918\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 549: 0.9971575207264114\n","Performing step 550 of gradient descent.\n","z [ 4.53570405  3.20777812  2.09329843 ... -5.58861111  1.18663791\n","  2.14720912] (12665,)\n","y hat: [0.98939433 0.96112593 0.89025011 ... 0.00372628 0.76613922 0.89540769]\n","Loss at step 550: -4.990595878816481\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 550: 0.9971575207264114\n","Performing step 551 of gradient descent.\n","z [ 4.53806517  3.20928454  2.09451998 ... -5.59178964  1.18746491\n","  2.14851767] (12665,)\n","y hat: [0.98941908 0.96118218 0.89036941 ... 0.0037145  0.76628736 0.89553018]\n","Loss at step 551: -4.984020209029233\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 551: 0.9971575207264114\n","Performing step 552 of gradient descent.\n","z [ 4.54042218  3.21078814  2.09573959 ... -5.59496308  1.18829073\n","  2.14982428] (12665,)\n","y hat: [0.98944372 0.96123824 0.8904884  ... 0.00370278 0.76643522 0.89565236]\n","Loss at step 552: -4.977466533124181\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 552: 0.9971575207264114\n","Performing step 553 of gradient descent.\n","z [ 4.5427751   3.21228895  2.09695726 ... -5.59813144  1.18911537\n","  2.15112896] (12665,)\n","y hat: [0.98946827 0.96129412 0.89060709 ... 0.00369111 0.76658281 0.89577423]\n","Loss at step 553: -4.970934737297945\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 553: 0.9971575207264114\n","Performing step 554 of gradient descent.\n","z [ 4.54512394  3.21378697  2.09817299 ... -5.60129475  1.18993882\n","  2.1524317 ] (12665,)\n","y hat: [0.98949272 0.96134982 0.89072548 ... 0.00367949 0.76673012 0.89589579]\n","Loss at step 554: -4.96442470854088\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 554: 0.9971575207264114\n","Performing step 555 of gradient descent.\n","z [ 4.54746872  3.21528221  2.0993868  ... -5.60445301  1.1907611\n","  2.15373252] (12665,)\n","y hat: [0.98951707 0.96140534 0.89084356 ... 0.00366793 0.76687716 0.89601705]\n","Loss at step 555: -4.957936334629757\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 555: 0.9971575207264114\n","Performing step 556 of gradient descent.\n","z [ 4.54980945  3.21677468  2.10059869 ... -5.60760625  1.19158221\n","  2.15503142] (12665,)\n","y hat: [0.98954132 0.96146068 0.89096135 ... 0.00365643 0.76702392 0.89613801]\n","Loss at step 556: -4.951469504121515\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 556: 0.9971575207264114\n","Performing step 557 of gradient descent.\n","z [ 4.55214615  3.2182644   2.10180866 ... -5.61075449  1.19240215\n","  2.15632841] (12665,)\n","y hat: [0.98956548 0.96151584 0.89107885 ... 0.00364497 0.76717041 0.89625866]\n","Loss at step 557: -4.945024106345821\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 557: 0.9971575207264114\n","Performing step 558 of gradient descent.\n","z [ 4.55447882  3.21975136  2.10301673 ... -5.61389773  1.19322092\n","  2.15762349] (12665,)\n","y hat: [0.98958954 0.96157083 0.89119604 ... 0.00363358 0.76731663 0.89637902]\n","Loss at step 558: -4.938600031398738\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 558: 0.9971575207264114\n","Performing step 559 of gradient descent.\n","z [ 4.55680749  3.22123559  2.10422289 ... -5.617036    1.19403854\n","  2.15891668] (12665,)\n","y hat: [0.9896135  0.96162564 0.89131294 ... 0.00362223 0.76746258 0.89649907]\n","Loss at step 559: -4.9321971701358\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 559: 0.9971575207264114\n","Performing step 560 of gradient descent.\n","z [ 4.55913217  3.22271709  2.10542717 ... -5.62016931  1.194855\n","  2.16020798] (12665,)\n","y hat: [0.98963737 0.96168027 0.89142955 ... 0.00361094 0.76760825 0.89661883]\n","Loss at step 560: -4.92581541416577\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 560: 0.9971575207264114\n","Performing step 561 of gradient descent.\n","z [ 4.56145286  3.22419587  2.10662955 ... -5.62329767  1.19567032\n","  2.16149739] (12665,)\n","y hat: [0.98966114 0.96173473 0.89154587 ... 0.0035997  0.76775366 0.89673829]\n","Loss at step 561: -4.919454655843635\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 561: 0.9971575207264114\n","Performing step 562 of gradient descent.\n","z [ 4.5637696   3.22567194  2.10783006 ... -5.62642111  1.19648448\n","  2.16278492] (12665,)\n","y hat: [0.98968482 0.96178901 0.89166189 ... 0.00358852 0.7678988  0.89685745]\n","Loss at step 562: -4.913114788264673\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 562: 0.9971575207264114\n","Performing step 563 of gradient descent.\n","z [ 4.56608238  3.22714531  2.10902869 ... -5.62953964  1.19729751\n","  2.16407058] (12665,)\n","y hat: [0.9897084  0.96184312 0.89177763 ... 0.00357738 0.76804368 0.89697632]\n","Loss at step 563: -4.9067957052575775\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 563: 0.9971575207264114\n","Performing step 564 of gradient descent.\n","z [ 4.56839123  3.22861599  2.11022545 ... -5.63265328  1.19810939\n","  2.16535437] (12665,)\n","y hat: [0.98973189 0.96189706 0.89189307 ... 0.0035663  0.76818828 0.89709489]\n","Loss at step 564: -4.90049730137852\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 564: 0.9971575207264114\n","Performing step 565 of gradient descent.\n","z [ 4.57069615  3.23008399  2.11142035 ... -5.63576204  1.19892014\n","  2.16663631] (12665,)\n","y hat: [0.98975529 0.96195083 0.89200823 ... 0.00355527 0.76833263 0.89721318]\n","Loss at step 565: -4.89421947190471\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 565: 0.9971575207264114\n","Performing step 566 of gradient descent.\n","z [ 4.57299717  3.23154932  2.11261339 ... -5.63886593  1.19972976\n","  2.16791638] (12665,)\n","y hat: [0.98977859 0.96200442 0.8921231  ... 0.00354429 0.76847671 0.89733117]\n","Loss at step 566: -4.887962112828176\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 566: 0.9971575207264114\n","Performing step 567 of gradient descent.\n","z [ 4.57529429  3.23301199  2.11380459 ... -5.64196498  1.20053825\n","  2.16919462] (12665,)\n","y hat: [0.98980181 0.96205785 0.89223769 ... 0.00353337 0.76862052 0.89744887]\n","Loss at step 567: -4.881725120849668\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 567: 0.9971575207264114\n","Performing step 568 of gradient descent.\n","z [ 4.57758753  3.23447201  2.11499394 ... -5.6450592   1.20134562\n","  2.17047101] (12665,)\n","y hat: [0.98982493 0.96211111 0.89235199 ... 0.00352249 0.76876408 0.89756628]\n","Loss at step 568: -4.875508393372762\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 568: 0.9971575207264114\n","Performing step 569 of gradient descent.\n","z [ 4.5798769   3.23592939  2.11618146 ... -5.6481486   1.20215187\n","  2.17174556] (12665,)\n","y hat: [0.98984796 0.9621642  0.89246601 ... 0.00351166 0.76890737 0.8976834 ]\n","Loss at step 569: -4.8693118284975485\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 569: 0.9971575207264114\n","Performing step 570 of gradient descent.\n","z [ 4.58216241  3.23738413  2.11736714 ... -5.6512332   1.20295701\n","  2.17301829] (12665,)\n","y hat: [0.9898709  0.96221712 0.89257975 ... 0.00350088 0.7690504  0.89780024]\n","Loss at step 570: -4.863135325014969\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 570: 0.9971575207264114\n","Performing step 571 of gradient descent.\n","z [ 4.58444408  3.23883625  2.118551   ... -5.65431301  1.20376104\n","  2.1742892 ] (12665,)\n","y hat: [0.98989376 0.96226988 0.89269321 ... 0.00349015 0.76919318 0.8979168 ]\n","Loss at step 571: -4.856978782400785\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 571: 0.9971575207264114\n","Performing step 572 of gradient descent.\n","z [ 4.58672193  3.24028576  2.11973305 ... -5.65738805  1.20456395\n","  2.17555828] (12665,)\n","y hat: [0.98991652 0.96232247 0.89280638 ... 0.00347948 0.76933569 0.89803306]\n","Loss at step 572: -4.850842100809744\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 572: 0.9971575207264114\n","Performing step 573 of gradient descent.\n","z [ 4.58899596  3.24173266  2.12091328 ... -5.66045834  1.20536577\n","  2.17682556] (12665,)\n","y hat: [0.98993919 0.9623749  0.89291928 ... 0.00346885 0.76947795 0.89814905]\n","Loss at step 573: -4.844725181069979\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 573: 0.9971575207264114\n","Performing step 574 of gradient descent.\n","z [ 4.59126618  3.24317696  2.1220917  ... -5.66352389  1.20616648\n","  2.17809103] (12665,)\n","y hat: [0.98996178 0.96242716 0.89303191 ... 0.00345827 0.76961995 0.89826475]\n","Loss at step 574: -4.838627924677072\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 574: 0.9971575207264114\n","Performing step 575 of gradient descent.\n","z [ 4.59353262  3.24461868  2.12326832 ... -5.66658472  1.2069661\n","  2.1793547 ] (12665,)\n","y hat: [0.98998427 0.96247926 0.89314425 ... 0.00344773 0.7697617  0.89838018]\n","Loss at step 575: -4.832550233788673\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 575: 0.9971575207264114\n","Performing step 576 of gradient descent.\n","z [ 4.59579528  3.24605782  2.12444315 ... -5.66964083  1.20776463\n","  2.18061658] (12665,)\n","y hat: [0.99000668 0.9625312  0.89325632 ... 0.00343725 0.76990319 0.89849532]\n","Loss at step 576: -4.826492011218617\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 576: 0.9971575207264114\n","Performing step 577 of gradient descent.\n","z [ 4.59805418  3.24749439  2.12561619 ... -5.67269226  1.20856207\n","  2.18187668] (12665,)\n","y hat: [0.99002901 0.96258297 0.89336812 ... 0.00342681 0.77004442 0.89861018]\n","Loss at step 577: -4.820453160431783\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 577: 0.9971575207264114\n","Performing step 578 of gradient descent.\n","z [ 4.60030932  3.2489284   2.12678745 ... -5.675739    1.20935842\n","  2.18313499] (12665,)\n","y hat: [0.99005125 0.96263459 0.89347964 ... 0.00341642 0.77018541 0.89872477]\n","Loss at step 578: -4.814433585538131\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 578: 0.9971575207264114\n","Performing step 579 of gradient descent.\n","z [ 4.60256073  3.25035986  2.12795693 ... -5.67878108  1.21015369\n","  2.18439153] (12665,)\n","y hat: [0.9900734  0.96268604 0.8935909  ... 0.00340608 0.77032614 0.89883908]\n","Loss at step 579: -4.8084331912877225\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 579: 0.9971575207264114\n","Performing step 580 of gradient descent.\n","z [ 4.60480842  3.25178878  2.12912465 ... -5.6818185   1.21094789\n","  2.18564629] (12665,)\n","y hat: [0.99009546 0.96273734 0.89370188 ... 0.00339579 0.77046662 0.89895312]\n","Loss at step 580: -4.8024518830651495\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 580: 0.9971575207264114\n","Performing step 581 of gradient descent.\n","z [ 4.60705239  3.25321517  2.13029059 ... -5.68485129  1.21174101\n","  2.1868993 ] (12665,)\n","y hat: [0.99011744 0.96278847 0.89381259 ... 0.00338554 0.77060686 0.89906688]\n","Loss at step 581: -4.796489566884204\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 581: 0.9971575207264114\n","Performing step 582 of gradient descent.\n","z [ 4.60929266  3.25463903  2.13145478 ... -5.68787946  1.21253306\n","  2.18815054] (12665,)\n","y hat: [0.99013934 0.96283945 0.89392304 ... 0.00337534 0.77074684 0.89918037]\n","Loss at step 582: -4.790546149382722\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 582: 0.9971575207264114\n","Performing step 583 of gradient descent.\n","z [ 4.61152924  3.25606037  2.13261722 ... -5.69090303  1.21332405\n","  2.18940004] (12665,)\n","y hat: [0.99016115 0.96289027 0.89403321 ... 0.00336518 0.77088657 0.89929358]\n","Loss at step 583: -4.784621537817324\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 583: 0.9971575207264114\n","Performing step 584 of gradient descent.\n","z [ 4.61376215  3.25747921  2.1337779  ... -5.693922    1.21411397\n","  2.19064778] (12665,)\n","y hat: [0.99018288 0.96294094 0.89414312 ... 0.00335507 0.77102606 0.89940653]\n","Loss at step 584: -4.778715640058237\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 584: 0.9971575207264114\n","Performing step 585 of gradient descent.\n","z [ 4.61599139  3.25889555  2.13493685 ... -5.6969364   1.21490283\n","  2.19189379] (12665,)\n","y hat: [0.99020453 0.96299145 0.89425277 ... 0.00334501 0.7711653  0.89951921]\n","Loss at step 585: -4.772828364584305\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 585: 0.9971575207264114\n","Performing step 586 of gradient descent.\n","z [ 4.61821699  3.2603094   2.13609406 ... -5.69994623  1.21569064\n","  2.19313806] (12665,)\n","y hat: [0.99022609 0.9630418  0.89436215 ... 0.00333499 0.77130429 0.89963161]\n","Loss at step 586: -4.766959620477777\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 586: 0.9971575207264114\n","Performing step 587 of gradient descent.\n","z [ 4.62043895  3.26172077  2.13724954 ... -5.70295151  1.2164774\n","  2.1943806 ] (12665,)\n","y hat: [0.99024757 0.96309201 0.89447127 ... 0.00332501 0.77144304 0.89974375]\n","Loss at step 587: -4.761109317419527\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 587: 0.9971575207264114\n","Performing step 588 of gradient descent.\n","z [ 4.62265728  3.26312967  2.13840329 ... -5.70595226  1.21726311\n","  2.19562142] (12665,)\n","y hat: [0.99026897 0.96314205 0.89458013 ... 0.00331508 0.77158155 0.89985562]\n","Loss at step 588: -4.75527736568383\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 588: 0.9971575207264114\n","Performing step 589 of gradient descent.\n","z [ 4.62487199  3.2645361   2.13955533 ... -5.70894848  1.21804777\n","  2.19686052] (12665,)\n","y hat: [0.99029029 0.96319195 0.89468872 ... 0.0033052  0.77171981 0.89996723]\n","Loss at step 589: -4.749463676133666\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 589: 0.9971575207264114\n","Performing step 590 of gradient descent.\n","z [ 4.62708311  3.26594008  2.14070565 ... -5.7119402   1.21883139\n","  2.19809791] (12665,)\n","y hat: [0.99031153 0.96324169 0.89479706 ... 0.00329536 0.77185783 0.90007857]\n","Loss at step 590: -4.74366816021582\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 590: 0.9971575207264114\n","Performing step 591 of gradient descent.\n","z [ 4.62929064  3.2673416   2.14185427 ... -5.71492743  1.21961398\n","  2.19933359] (12665,)\n","y hat: [0.99033269 0.96329128 0.89490513 ... 0.00328556 0.77199561 0.90018965]\n","Loss at step 591: -4.7378907299561375\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 591: 0.9972364784840111\n","Performing step 592 of gradient descent.\n","z [ 4.63149458  3.26874069  2.14300118 ... -5.71791018  1.22039553\n","  2.20056756] (12665,)\n","y hat: [0.99035377 0.96334072 0.89501295 ... 0.00327581 0.77213315 0.90030047]\n","Loss at step 592: -4.73213129795457\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 592: 0.9972364784840111\n","Performing step 593 of gradient descent.\n","z [ 4.63369497  3.27013735  2.14414639 ... -5.72088846  1.22117604\n","  2.20179984] (12665,)\n","y hat: [0.99037476 0.96339002 0.89512051 ... 0.0032661  0.77227044 0.90041102]\n","Loss at step 593: -4.726389777380752\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 593: 0.9972364784840111\n","Performing step 594 of gradient descent.\n","z [ 4.6358918   3.27153159  2.14528992 ... -5.72386229  1.22195553\n","  2.20303043] (12665,)\n","y hat: [0.99039568 0.96343916 0.89522782 ... 0.00325643 0.7724075  0.90052131]\n","Loss at step 594: -4.720666081969053\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 594: 0.9972364784840111\n","Performing step 595 of gradient descent.\n","z [ 4.63808508  3.27292341  2.14643176 ... -5.72683169  1.222734\n","  2.20425934] (12665,)\n","y hat: [0.99041652 0.96348815 0.89533487 ... 0.00324681 0.77254432 0.90063135]\n","Loss at step 595: -4.714960126014124\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 595: 0.9972364784840111\n","Performing step 596 of gradient descent.\n","z [ 4.64027484  3.27431283  2.14757191 ... -5.72979666  1.22351145\n","  2.20548656] (12665,)\n","y hat: [0.99043728 0.963537   0.89544166 ... 0.00323722 0.77268091 0.90074113]\n","Loss at step 596: -4.70927182436643\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 596: 0.9972364784840111\n","Performing step 597 of gradient descent.\n","z [ 4.64246107  3.27569985  2.1487104  ... -5.73275722  1.22428787\n","  2.20671211] (12665,)\n","y hat: [0.99045797 0.9635857  0.89554821 ... 0.00322768 0.77281726 0.90085064]\n","Loss at step 597: -4.70360109242741\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 597: 0.9972364784840111\n","Performing step 598 of gradient descent.\n","z [ 4.6446438   3.27708447  2.14984721 ... -5.73571338  1.22506329\n","  2.20793599] (12665,)\n","y hat: [0.99047858 0.96363425 0.8956545  ... 0.00321819 0.77295337 0.90095991]\n","Loss at step 598: -4.697947846145355\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 598: 0.9972364784840111\n","Performing step 599 of gradient descent.\n","z [ 4.64682304  3.27846672  2.15098236 ... -5.73866516  1.22583769\n","  2.20915821] (12665,)\n","y hat: [0.99049911 0.96368266 0.89576054 ... 0.00320873 0.77308924 0.90106891]\n","Loss at step 599: -4.692312002010578\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 599: 0.9972364784840111\n","Performing step 600 of gradient descent.\n","z [ 4.64899878  3.27984659  2.15211584 ... -5.74161257  1.22661109\n","  2.21037876] (12665,)\n","y hat: [0.99051956 0.96373092 0.89586633 ... 0.00319932 0.77322488 0.90117766]\n","Loss at step 600: -4.68669347705152\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 600: 0.9972364784840111\n","Performing step 601 of gradient descent.\n","z [ 4.65117106  3.2812241   2.15324768 ... -5.74455562  1.22738348\n","  2.21159766] (12665,)\n","y hat: [0.99053994 0.96377904 0.89597187 ... 0.00318995 0.77336029 0.90128616]\n","Loss at step 601: -4.681092188829916\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 601: 0.9972364784840111\n","Performing step 602 of gradient descent.\n","z [ 4.65333987  3.28259925  2.15437786 ... -5.74749433  1.22815487\n","  2.21281492] (12665,)\n","y hat: [0.99056024 0.96382701 0.89607716 ... 0.00318062 0.77349547 0.90139441]\n","Loss at step 602: -4.675508055436696\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 602: 0.9972364784840111\n","Performing step 603 of gradient descent.\n","z [ 4.65550524  3.28397204  2.15550641 ... -5.75042871  1.22892526\n","  2.21403053] (12665,)\n","y hat: [0.99058046 0.96387485 0.89618221 ... 0.00317133 0.77363041 0.9015024 ]\n","Loss at step 603: -4.669940995487838\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 603: 0.9972364784840111\n","Performing step 604 of gradient descent.\n","z [ 4.65766716  3.2853425   2.15663331 ... -5.75335877  1.22969466\n","  2.2152445 ] (12665,)\n","y hat: [0.99060061 0.96392253 0.89628701 ... 0.00316208 0.77376513 0.90161014]\n","Loss at step 604: -4.664390928119811\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 604: 0.9972364784840111\n","Performing step 605 of gradient descent.\n","z [ 4.65982566  3.28671062  2.15775858 ... -5.75628453  1.23046307\n","  2.21645684] (12665,)\n","y hat: [0.99062069 0.96397008 0.89639156 ... 0.00315287 0.77389961 0.90171764]\n","Loss at step 605: -4.658857772985854\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 605: 0.9972364784840111\n","Performing step 606 of gradient descent.\n","z [ 4.66198074  3.28807642  2.15888223 ... -5.75920599  1.23123049\n","  2.21766755] (12665,)\n","y hat: [0.99064069 0.96401749 0.89649588 ... 0.0031437  0.77403387 0.90182488]\n","Loss at step 606: -4.653341450251309\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 606: 0.9972364784840111\n","Performing step 607 of gradient descent.\n","z [ 4.66413242  3.28943989  2.16000425 ... -5.76212318  1.23199693\n","  2.21887664] (12665,)\n","y hat: [0.99066062 0.96406475 0.89659994 ... 0.00313457 0.77416789 0.90193188]\n","Loss at step 607: -4.647841880589975\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 607: 0.9972364784840111\n","Performing step 608 of gradient descent.\n","z [ 4.6662807   3.29080106  2.16112465 ... -5.7650361   1.23276238\n","  2.22008411] (12665,)\n","y hat: [0.99068048 0.96411188 0.89670377 ... 0.00312548 0.77430169 0.90203863]\n","Loss at step 608: -4.642358985179758\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 608: 0.9972364784840111\n","Performing step 609 of gradient descent.\n","z [ 4.6684256   3.29215992  2.16224344 ... -5.76794477  1.23352686\n","  2.22128997] (12665,)\n","y hat: [0.99070026 0.96415887 0.89680735 ... 0.00311643 0.77443526 0.90214513]\n","Loss at step 609: -4.636892685698726\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 609: 0.9972364784840111\n","Performing step 610 of gradient descent.\n","z [ 4.67056712  3.29351648  2.16336062 ... -5.7708492   1.23429036\n","  2.22249422] (12665,)\n","y hat: [0.99071997 0.96420572 0.89691069 ... 0.00310742 0.77456861 0.90225139]\n","Loss at step 610: -4.63144290432114\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 610: 0.9972364784840111\n","Performing step 611 of gradient descent.\n","z [ 4.67270529  3.29487076  2.1644762  ... -5.77374941  1.23505289\n","  2.22369686] (12665,)\n","y hat: [0.99073961 0.96425243 0.8970138  ... 0.00309845 0.77470172 0.90235741]\n","Loss at step 611: -4.626009563713563\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 611: 0.9972364784840111\n","Performing step 612 of gradient descent.\n","z [ 4.6748401   3.29622275  2.16559019 ... -5.7766454   1.23581445\n","  2.22489791] (12665,)\n","y hat: [0.99075917 0.964299   0.89711666 ... 0.00308952 0.77483462 0.90246318]\n","Loss at step 612: -4.620592587030749\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 612: 0.9972364784840111\n","Performing step 613 of gradient descent.\n","z [ 4.67697157  3.29757248  2.16670258 ... -5.77953719  1.23657504\n","  2.22609737] (12665,)\n","y hat: [0.99077867 0.96434544 0.89721929 ... 0.00308063 0.77496729 0.90256871]\n","Loss at step 613: -4.615191897911987\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 613: 0.9972364784840111\n","Performing step 614 of gradient descent.\n","z [ 4.67909971  3.29891993  2.16781338 ... -5.78242479  1.23733468\n","  2.22729524] (12665,)\n","y hat: [0.99079809 0.96439174 0.89732168 ... 0.00307177 0.77509974 0.90267399]\n","Loss at step 614: -4.609807420477193\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 614: 0.9972364784840111\n","Performing step 615 of gradient descent.\n","z [ 4.68122454  3.30026513  2.1689226  ... -5.78530821  1.23809335\n","  2.22849152] (12665,)\n","y hat: [0.99081744 0.96443791 0.89742383 ... 0.00306295 0.77523196 0.90277904]\n","Loss at step 615: -4.604439079323047\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 615: 0.9972364784840111\n","Performing step 616 of gradient descent.\n","z [ 4.68334605  3.30160808  2.17003025 ... -5.78818747  1.23885107\n","  2.22968623] (12665,)\n","y hat: [0.99083672 0.96448394 0.89752575 ... 0.00305417 0.77536396 0.90288385]\n","Loss at step 616: -4.599086799519296\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 616: 0.9972364784840111\n","Performing step 617 of gradient descent.\n","z [ 4.68546427  3.30294878  2.17113632 ... -5.79106257  1.23960783\n","  2.23087936] (12665,)\n","y hat: [0.99085594 0.96452983 0.89762743 ... 0.00304543 0.77549574 0.90298842]\n","Loss at step 617: -4.593750506605022\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 617: 0.9972364784840111\n","Performing step 618 of gradient descent.\n","z [ 4.6875792   3.30428725  2.17224082 ... -5.79393354  1.24036364\n","  2.23207093] (12665,)\n","y hat: [0.99087508 0.9645756  0.89772888 ... 0.00303673 0.7756273  0.90309275]\n","Loss at step 618: -4.588430126584888\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 618: 0.9972364784840111\n","Performing step 619 of gradient descent.\n","z [ 4.68969086  3.30562348  2.17334376 ... -5.79680038  1.24111851\n","  2.23326093] (12665,)\n","y hat: [0.99089415 0.96462123 0.8978301  ... 0.00302806 0.77575865 0.90319684]\n","Loss at step 619: -4.583125585925581\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 619: 0.9972364784840111\n","Performing step 620 of gradient descent.\n","z [ 4.69179925  3.3069575   2.17444514 ... -5.79966311  1.24187243\n","  2.23444937] (12665,)\n","y hat: [0.99091316 0.96466672 0.89793109 ... 0.00301943 0.77588977 0.9033007 ]\n","Loss at step 620: -4.577836811552103\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 620: 0.9973154362416108\n","Performing step 621 of gradient descent.\n","z [ 4.69390438  3.3082893   2.17554497 ... -5.80252173  1.24262541\n","  2.23563626] (12665,)\n","y hat: [0.99093209 0.96471209 0.89803184 ... 0.00301084 0.77602067 0.90340433]\n","Loss at step 621: -4.572563730844188\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 621: 0.9973154362416108\n","Performing step 622 of gradient descent.\n","z [ 4.69600627  3.30961889  2.17664325 ... -5.80537626  1.24337745\n","  2.2368216 ] (12665,)\n","y hat: [0.99095096 0.96475733 0.89813237 ... 0.00300228 0.77615136 0.90350772]\n","Loss at step 622: -4.567306271632845\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 622: 0.9973154362416108\n","Performing step 623 of gradient descent.\n","z [ 4.69810492  3.31094629  2.17773999 ... -5.80822672  1.24412856\n","  2.2380054 ] (12665,)\n","y hat: [0.99096976 0.96480243 0.89823267 ... 0.00299376 0.77628183 0.90361087]\n","Loss at step 623: -4.562064362196646\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 623: 0.9973154362416108\n","Performing step 624 of gradient descent.\n","z [ 4.70020035  3.31227149  2.17883518 ... -5.8110731   1.24487873\n","  2.23918765] (12665,)\n","y hat: [0.99098849 0.9648474  0.89833274 ... 0.00298528 0.77641209 0.90371379]\n","Loss at step 624: -4.556837931258453\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 624: 0.9973154362416108\n","Performing step 625 of gradient descent.\n","z [ 4.70229256  3.3135945   2.17992885 ... -5.81391543  1.24562797\n","  2.24036837] (12665,)\n","y hat: [0.99100716 0.96489225 0.89843258 ... 0.00297683 0.77654213 0.90381649]\n","Loss at step 625: -4.551626907981856\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 625: 0.9973154362416108\n","Performing step 626 of gradient descent.\n","z [ 4.70438157  3.31491534  2.18102098 ... -5.81675372  1.24637629\n","  2.24154756] (12665,)\n","y hat: [0.99102575 0.96493697 0.89853219 ... 0.00296842 0.77667195 0.90391895]\n","Loss at step 626: -4.546431221967729\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 626: 0.9973154362416108\n","Performing step 627 of gradient descent.\n","z [ 4.70646738  3.316234    2.18211159 ... -5.81958798  1.24712369\n","  2.24272522] (12665,)\n","y hat: [0.99104429 0.96498155 0.89863158 ... 0.00296004 0.77680156 0.90402118]\n","Loss at step 627: -4.541250803250815\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 627: 0.9973154362416108\n","Performing step 628 of gradient descent.\n","z [ 4.70855001  3.3175505   2.18320067 ... -5.82241822  1.24787016\n","  2.24390136] (12665,)\n","y hat: [0.99106275 0.96502601 0.89873075 ... 0.0029517  0.77693096 0.90412318]\n","Loss at step 628: -4.5360855822964385\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 628: 0.9973154362416108\n","Performing step 629 of gradient descent.\n","z [ 4.71062946  3.31886484  2.18428824 ... -5.82524445  1.24861572\n","  2.24507599] (12665,)\n","y hat: [0.99108115 0.96507035 0.89882969 ... 0.00294339 0.77706014 0.90422495]\n","Loss at step 629: -4.530935489997268\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 629: 0.9973154362416108\n","Performing step 630 of gradient descent.\n","z [ 4.71270575  3.32017703  2.1853743  ... -5.82806668  1.24936036\n","  2.2462491 ] (12665,)\n","y hat: [0.99109948 0.96511455 0.89892841 ... 0.00293512 0.77718912 0.9043265 ]\n","Loss at step 630: -4.5258004576696615\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 630: 0.9973154362416108\n","Performing step 631 of gradient descent.\n","z [ 4.71477888  3.32148707  2.18645886 ... -5.83088494  1.25010409\n","  2.2474207 ] (12665,)\n","y hat: [0.99111775 0.96515863 0.8990269  ... 0.00292689 0.77731788 0.90442782]\n","Loss at step 631: -4.520680417050946\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 631: 0.9973154362416108\n","Performing step 632 of gradient descent.\n","z [ 4.71684887  3.32279498  2.18754191 ... -5.83369921  1.25084691\n","  2.24859081] (12665,)\n","y hat: [0.99113596 0.96520259 0.89912518 ... 0.00291868 0.77744643 0.90452891]\n","Loss at step 632: -4.515575300295589\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 632: 0.9973154362416108\n","Performing step 633 of gradient descent.\n","z [ 4.71891573  3.32410075  2.18862346 ... -5.83650953  1.25158882\n","  2.24975941] (12665,)\n","y hat: [0.9911541  0.96524642 0.89922323 ... 0.00291052 0.77757477 0.90462978]\n","Loss at step 633: -4.510485039972422\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 633: 0.9973154362416108\n","Performing step 634 of gradient descent.\n","z [ 4.72097945  3.3254044   2.18970353 ... -5.8393159   1.25232983\n","  2.25092652] (12665,)\n","y hat: [0.99117217 0.96529012 0.89932107 ... 0.00290238 0.77770291 0.90473042]\n","Loss at step 634: -4.505409569061289\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 634: 0.9973154362416108\n","Performing step 635 of gradient descent.\n","z [ 4.72304007  3.32670594  2.1907821  ... -5.84211833  1.25306994\n","  2.25209213] (12665,)\n","y hat: [0.99119019 0.9653337  0.89941868 ... 0.00289429 0.77783083 0.90483085]\n","Loss at step 635: -4.500348820949969\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 635: 0.9973154362416108\n","Performing step 636 of gradient descent.\n","z [ 4.72509757  3.32800536  2.19185919 ... -5.84491683  1.25380915\n","  2.25325627] (12665,)\n","y hat: [0.99120813 0.96537716 0.89951608 ... 0.00288622 0.77795855 0.90493104]\n","Loss at step 636: -4.495302729430909\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 636: 0.9973154362416108\n","Performing step 637 of gradient descent.\n","z [ 4.72715198  3.32930268  2.1929348  ... -5.84771142  1.25454746\n","  2.25441892] (12665,)\n","y hat: [0.99122602 0.9654205  0.89961326 ... 0.00287819 0.77808606 0.90503102]\n","Loss at step 637: -4.490271228698351\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 637: 0.9973154362416108\n","Performing step 638 of gradient descent.\n","z [ 4.72920331  3.33059791  2.19400893 ... -5.8505021   1.25528489\n","  2.25558009] (12665,)\n","y hat: [0.99124384 0.96546371 0.89971022 ... 0.00287019 0.77821336 0.90513078]\n","Loss at step 638: -4.485254253345052\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 638: 0.9973154362416108\n","Performing step 639 of gradient descent.\n","z [ 4.73125155  3.33189104  2.1950816  ... -5.85328889  1.25602142\n","  2.2567398 ] (12665,)\n","y hat: [0.9912616  0.9655068  0.89980697 ... 0.00286223 0.77834046 0.90523031]\n","Loss at step 639: -4.480251738359474\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 639: 0.9973154362416108\n","Performing step 640 of gradient descent.\n","z [ 4.73329673  3.33318209  2.19615279 ... -5.8560718   1.25675706\n","  2.25789803] (12665,)\n","y hat: [0.9912793  0.96554977 0.8999035  ... 0.0028543  0.77846735 0.90532963]\n","Loss at step 640: -4.475263619122554\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 640: 0.9973154362416108\n","Performing step 641 of gradient descent.\n","z [ 4.73533885  3.33447106  2.19722253 ... -5.85885083  1.25749183\n","  2.2590548 ] (12665,)\n","y hat: [0.99129693 0.96559262 0.89999982 ... 0.0028464  0.77859404 0.90542873]\n","Loss at step 641: -4.470289831404912\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 641: 0.9973154362416108\n","Performing step 642 of gradient descent.\n","z [ 4.73737792  3.33575796  2.19829081 ... -5.86162601  1.25822571\n","  2.26021012] (12665,)\n","y hat: [0.99131451 0.96563535 0.90009592 ... 0.00283853 0.77872052 0.90552761]\n","Loss at step 642: -4.46533031136371\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 642: 0.9973154362416108\n","Performing step 643 of gradient descent.\n","z [ 4.73941395  3.33704279  2.19935763 ... -5.86439734  1.25895871\n","  2.26136397] (12665,)\n","y hat: [0.99133202 0.96567796 0.90019181 ... 0.0028307  0.7788468  0.90562627]\n","Loss at step 643: -4.460384995539922\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 643: 0.9973154362416108\n","Performing step 644 of gradient descent.\n","z [ 4.74144695  3.33832557  2.20042301 ... -5.86716483  1.25969083\n","  2.26251638] (12665,)\n","y hat: [0.99134947 0.96572045 0.90028749 ... 0.0028229  0.77897288 0.90572472]\n","Loss at step 644: -4.455453820855385\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 644: 0.9973154362416108\n","Performing step 645 of gradient descent.\n","z [ 4.74347693  3.3396063   2.20148694 ... -5.86992849  1.26042208\n","  2.26366734] (12665,)\n","y hat: [0.99136686 0.96576283 0.90038296 ... 0.00281513 0.77909876 0.90582295]\n","Loss at step 645: -4.4505367246097425\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 645: 0.9973154362416108\n","Performing step 646 of gradient descent.\n","z [ 4.74550389  3.34088498  2.20254943 ... -5.87268834  1.26115246\n","  2.26481686] (12665,)\n","y hat: [0.9913842  0.96580508 0.90047822 ... 0.00280739 0.77922443 0.90592097]\n","Loss at step 646: -4.44563364447788\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 646: 0.9973154362416108\n","Performing step 647 of gradient descent.\n","z [ 4.74752786  3.34216162  2.20361049 ... -5.87544438  1.26188198\n","  2.26596494] (12665,)\n","y hat: [0.99140147 0.96584722 0.90057327 ... 0.00279969 0.77934991 0.90601877]\n","Loss at step 647: -4.440744518506906\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 647: 0.9973154362416108\n","Performing step 648 of gradient descent.\n","z [ 4.74954883  3.34343624  2.20467011 ... -5.87819663  1.26261062\n","  2.26711159] (12665,)\n","y hat: [0.99141868 0.96588924 0.9006681  ... 0.00279201 0.77947518 0.90611636]\n","Loss at step 648: -4.435869285113499\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 648: 0.9973154362416108\n","Performing step 649 of gradient descent.\n","z [ 4.75156681  3.34470882  2.2057283  ... -5.8809451   1.26333841\n","  2.26825681] (12665,)\n","y hat: [0.99143583 0.96593114 0.90076274 ... 0.00278437 0.77960026 0.90621374]\n","Loss at step 649: -4.431007883080948\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 649: 0.9973154362416108\n","Performing step 650 of gradient descent.\n","z [ 4.75358182  3.34597939  2.20678508 ... -5.88368979  1.26406533\n","  2.2694006 ] (12665,)\n","y hat: [0.99145292 0.96597293 0.90085716 ... 0.00277676 0.77972514 0.9063109 ]\n","Loss at step 650: -4.4261602515566185\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 650: 0.9973154362416108\n","Performing step 651 of gradient descent.\n","z [ 4.75559386  3.34724794  2.20784043 ... -5.88643072  1.2647914\n","  2.27054297] (12665,)\n","y hat: [0.99146995 0.9660146  0.90095138 ... 0.00276918 0.77984982 0.90640786]\n","Loss at step 651: -4.421326330049084\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 651: 0.9973154362416108\n","Performing step 652 of gradient descent.\n","z [ 4.75760295  3.34851448  2.20889437 ... -5.8891679   1.26551661\n","  2.27168393] (12665,)\n","y hat: [0.99148693 0.96605616 0.90104539 ... 0.00276163 0.7799743  0.9065046 ]\n","Loss at step 652: -4.416506058425521\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 652: 0.9973154362416108\n","Performing step 653 of gradient descent.\n","z [ 4.75960909  3.34977902  2.20994689 ... -5.89190133  1.26624096\n","  2.27282347] (12665,)\n","y hat: [0.99150384 0.9660976  0.9011392  ... 0.00275411 0.78009858 0.90660114]\n","Loss at step 653: -4.411699376908969\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 653: 0.9973154362416108\n","Performing step 654 of gradient descent.\n","z [ 4.76161228  3.35104157  2.21099801 ... -5.89463104  1.26696447\n","  2.2739616 ] (12665,)\n","y hat: [0.9915207  0.96613893 0.9012328  ... 0.00274663 0.78022267 0.90669747]\n","Loss at step 654: -4.406906226075713\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 654: 0.9973154362416108\n","Performing step 655 of gradient descent.\n","z [ 4.76361255  3.35230213  2.21204773 ... -5.89735703  1.26768713\n","  2.27509833] (12665,)\n","y hat: [0.9915375  0.96618014 0.9013262  ... 0.00273917 0.78034657 0.90679359]\n","Loss at step 655: -4.402126546852738\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 655: 0.9973154362416108\n","Performing step 656 of gradient descent.\n","z [ 4.76560989  3.35356071  2.21309605 ... -5.90007931  1.26840895\n","  2.27623365] (12665,)\n","y hat: [0.99155425 0.96622124 0.90141939 ... 0.00273174 0.78047026 0.9068895 ]\n","Loss at step 656: -4.3973602805149214\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 656: 0.9973154362416108\n","Performing step 657 of gradient descent.\n","z [ 4.76760432  3.35481731  2.21414298 ... -5.90279788  1.26912992\n","  2.27736758] (12665,)\n","y hat: [0.99157093 0.96626223 0.90151238 ... 0.00272435 0.78059377 0.90698521]\n","Loss at step 657: -4.392607368682714\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 657: 0.9973154362416108\n","Performing step 658 of gradient descent.\n","z [ 4.76959585  3.35607194  2.21518851 ... -5.90551277  1.26985005\n","  2.27850012] (12665,)\n","y hat: [0.99158756 0.96630311 0.90160518 ... 0.00271698 0.78071708 0.90708071]\n","Loss at step 658: -4.387867753319372\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 658: 0.9973154362416108\n","Performing step 659 of gradient descent.\n","z [ 4.77158448  3.35732461  2.21623266 ... -5.90822398  1.27056935\n","  2.27963127] (12665,)\n","y hat: [0.99160413 0.96634387 0.90169777 ... 0.00270965 0.7808402  0.907176  ]\n","Loss at step 659: -4.383141376728565\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 659: 0.9973154362416108\n","Performing step 660 of gradient descent.\n","z [ 4.77357022  3.35857531  2.21727542 ... -5.91093152  1.27128781\n","  2.28076104] (12665,)\n","y hat: [0.99162065 0.96638453 0.90179016 ... 0.00270234 0.78096312 0.90727109]\n","Loss at step 660: -4.378428181551806\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 660: 0.9973154362416108\n","Performing step 661 of gradient descent.\n","z [ 4.77555309  3.35982407  2.21831681 ... -5.91363541  1.27200544\n","  2.28188942] (12665,)\n","y hat: [0.99163711 0.96642507 0.90188235 ... 0.00269506 0.78108585 0.90736598]\n","Loss at step 661: -4.373728110766033\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 661: 0.9973154362416108\n","Performing step 662 of gradient descent.\n","z [ 4.77753308  3.36107088  2.21935682 ... -5.91633564  1.27272223\n","  2.28301643] (12665,)\n","y hat: [0.99165351 0.9664655  0.90197434 ... 0.00268781 0.78120839 0.90746067]\n","Loss at step 662: -4.369041107680934\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 662: 0.9973154362416108\n","Performing step 663 of gradient descent.\n","z [ 4.77951021  3.36231575  2.22039546 ... -5.91903224  1.2734382\n","  2.28414206] (12665,)\n","y hat: [0.99166986 0.96650582 0.90206614 ... 0.0026806  0.78133074 0.90755515]\n","Loss at step 663: -4.364367115936831\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 663: 0.9973154362416108\n","Performing step 664 of gradient descent.\n","z [ 4.78148449  3.36355868  2.22143274 ... -5.92172521  1.27415335\n","  2.28526632] (12665,)\n","y hat: [0.99168616 0.96654604 0.90215774 ... 0.00267341 0.7814529  0.90764943]\n","Loss at step 664: -4.359706079501958\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 664: 0.9973154362416108\n","Performing step 665 of gradient descent.\n","z [ 4.78345592  3.36479968  2.22246865 ... -5.92441456  1.27486767\n","  2.28638922] (12665,)\n","y hat: [0.99170239 0.96658614 0.90224914 ... 0.00266624 0.78157487 0.90774351]\n","Loss at step 665: -4.355057942670305\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 665: 0.9973154362416108\n","Performing step 666 of gradient descent.\n","z [ 4.78542452  3.36603876  2.2235032  ... -5.92710031  1.27558117\n","  2.28751076] (12665,)\n","y hat: [0.99171858 0.96662614 0.90234034 ... 0.00265911 0.78169666 0.90783739]\n","Loss at step 666: -4.350422650058998\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 666: 0.9973154362416108\n","Performing step 667 of gradient descent.\n","z [ 4.78739029  3.36727593  2.2245364  ... -5.92978245  1.27629386\n","  2.28863094] (12665,)\n","y hat: [0.99173471 0.96666603 0.90243135 ... 0.00265201 0.78181825 0.90793107]\n","Loss at step 667: -4.345800146606151\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 667: 0.9973154362416108\n","Performing step 668 of gradient descent.\n","z [ 4.78935324  3.36851118  2.22556825 ... -5.93246101  1.27700573\n","  2.28974977] (12665,)\n","y hat: [0.99175078 0.96670581 0.90252217 ... 0.00264493 0.78193965 0.90802455]\n","Loss at step 668: -4.341190377568502\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 668: 0.9973154362416108\n","Performing step 669 of gradient descent.\n","z [ 4.79131338  3.36974453  2.22659875 ... -5.93513599  1.27771678\n","  2.29086725] (12665,)\n","y hat: [0.9917668  0.96674548 0.90261279 ... 0.00263789 0.78206087 0.90811784]\n","Loss at step 669: -4.3365932885188245\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 669: 0.9973154362416108\n","Performing step 670 of gradient descent.\n","z [ 4.79327072  3.37097598  2.22762791 ... -5.9378074   1.27842703\n","  2.29198338] (12665,)\n","y hat: [0.99178277 0.96678505 0.90270322 ... 0.00263087 0.7821819  0.90821093]\n","Loss at step 670: -4.33200882534413\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 670: 0.9973154362416108\n","Performing step 671 of gradient descent.\n","z [ 4.79522526  3.37220553  2.22865573 ... -5.94047525  1.27913646\n","  2.29309817] (12665,)\n","y hat: [0.99179868 0.96682451 0.90279345 ... 0.00262388 0.78230275 0.90830382]\n","Loss at step 671: -4.3274369342429315\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 671: 0.9973154362416108\n","Performing step 672 of gradient descent.\n","z [ 4.79717701  3.37343319  2.22968221 ... -5.94313956  1.27984509\n","  2.29421162] (12665,)\n","y hat: [0.99181454 0.96686386 0.9028835  ... 0.00261691 0.78242341 0.90839651]\n","Loss at step 672: -4.322877561723142\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 672: 0.9973154362416108\n","Performing step 673 of gradient descent.\n","z [ 4.79912599  3.37465898  2.23070736 ... -5.94580032  1.28055292\n","  2.29532374] (12665,)\n","y hat: [0.99183035 0.96690311 0.90297335 ... 0.00260998 0.78254388 0.90848901]\n","Loss at step 673: -4.3183306545999995\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 673: 0.9973154362416108\n","Performing step 674 of gradient descent.\n","z [ 4.80107219  3.37588288  2.23173119 ... -5.94845756  1.28125995\n","  2.29643453] (12665,)\n","y hat: [0.9918461  0.96694225 0.90306301 ... 0.00260307 0.78266417 0.90858132]\n","Loss at step 674: -4.3137961599935934\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 674: 0.9973154362416108\n","Performing step 675 of gradient descent.\n","z [ 4.80301564  3.37710491  2.23275368 ... -5.95111127  1.28196617\n","  2.29754399] (12665,)\n","y hat: [0.99186181 0.96698129 0.90315249 ... 0.00259619 0.78278428 0.90867343]\n","Loss at step 675: -4.30927402532681\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 675: 0.9973154362416108\n","Performing step 676 of gradient descent.\n","z [ 4.80495633  3.37832508  2.23377486 ... -5.95376148  1.2826716\n","  2.29865212] (12665,)\n","y hat: [0.99187746 0.96702023 0.90324177 ... 0.00258934 0.7829042  0.90876535]\n","Loss at step 676: -4.304764198323184\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 676: 0.9973154362416108\n","Performing step 677 of gradient descent.\n","z [ 4.80689427  3.37954339  2.23479472 ... -5.95640819  1.28337624\n","  2.29975894] (12665,)\n","y hat: [0.99189306 0.96705906 0.90333087 ... 0.00258251 0.78302394 0.90885707]\n","Loss at step 677: -4.3002666270046594\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 677: 0.9973154362416108\n","Performing step 678 of gradient descent.\n","z [ 4.80882947  3.38075984  2.23581327 ... -5.9590514   1.28408008\n","  2.30086444] (12665,)\n","y hat: [0.9919086  0.96709779 0.90341977 ... 0.00257571 0.7831435  0.90894861]\n","Loss at step 678: -4.295781259689384\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 678: 0.9973154362416108\n","Performing step 679 of gradient descent.\n","z [ 4.81076195  3.38197444  2.23683051 ... -5.96169113  1.28478314\n","  2.30196863] (12665,)\n","y hat: [0.9919241  0.96713642 0.90350849 ... 0.00256894 0.78326287 0.90903995]\n","Loss at step 679: -4.2913080449897745\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 679: 0.9973154362416108\n","Performing step 680 of gradient descent.\n","z [ 4.8126917   3.3831872   2.23784645 ... -5.96432739  1.28548541\n","  2.30307151] (12665,)\n","y hat: [0.99193954 0.96717494 0.90359703 ... 0.00256219 0.78338207 0.9091311 ]\n","Loss at step 680: -4.28684693181031\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 680: 0.9973154362416108\n","Performing step 681 of gradient descent.\n","z [ 4.81461874  3.38439812  2.23886108 ... -5.96696019  1.28618689\n","  2.30417308] (12665,)\n","y hat: [0.99195493 0.96721336 0.90368537 ... 0.00255547 0.78350108 0.90922206]\n","Loss at step 681: -4.282397869345368\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 681: 0.9973154362416108\n","Performing step 682 of gradient descent.\n","z [ 4.81654307  3.38560721  2.23987441 ... -5.96958953  1.28688759\n","  2.30527336] (12665,)\n","y hat: [0.99197028 0.96725168 0.90377354 ... 0.00254878 0.78361991 0.90931284]\n","Loss at step 682: -4.277960807077328\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 682: 0.9973154362416108\n","Performing step 683 of gradient descent.\n","z [ 4.8184647   3.38681447  2.24088645 ... -5.97221543  1.2875875\n","  2.30637233] (12665,)\n","y hat: [0.99198557 0.9672899  0.90386151 ... 0.00254211 0.78373857 0.90940342]\n","Loss at step 683: -4.273535694774332\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 683: 0.9973154362416108\n","Performing step 684 of gradient descent.\n","z [ 4.82038365  3.38801991  2.24189719 ... -5.97483789  1.28828664\n","  2.30747001] (12665,)\n","y hat: [0.99200081 0.96732802 0.90394931 ... 0.00253547 0.78385704 0.90949382]\n","Loss at step 684: -4.269122482488459\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 684: 0.9973154362416108\n","Performing step 685 of gradient descent.\n","z [ 4.82229991  3.38922353  2.24290665 ... -5.97745693  1.28898501\n","  2.30856641] (12665,)\n","y hat: [0.992016   0.96736604 0.90403692 ... 0.00252885 0.78397534 0.90958402]\n","Loss at step 685: -4.264721120553509\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 685: 0.9973154362416108\n","Performing step 686 of gradient descent.\n","z [ 4.82421349  3.39042534  2.24391483 ... -5.98007255  1.2896826\n","  2.30966151] (12665,)\n","y hat: [0.99203114 0.96740396 0.90412435 ... 0.00252226 0.78409346 0.90967405]\n","Loss at step 686: -4.26033155958313\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 686: 0.9973154362416108\n","Performing step 687 of gradient descent.\n","z [ 4.8261244   3.39162534  2.24492173 ... -5.98268477  1.29037941\n","  2.31075533] (12665,)\n","y hat: [0.99204624 0.96744178 0.90421159 ... 0.0025157  0.7842114  0.90976388]\n","Loss at step 687: -4.255953750468788\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 687: 0.9973154362416108\n","Performing step 688 of gradient descent.\n","z [ 4.82803266  3.39282354  2.24592734 ... -5.98529358  1.29107546\n","  2.31184787] (12665,)\n","y hat: [0.99206128 0.9674795  0.90429866 ... 0.00250916 0.78432917 0.90985353]\n","Loss at step 688: -4.251587644377773\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 688: 0.9973154362416108\n","Performing step 689 of gradient descent.\n","z [ 4.82993826  3.39401995  2.24693169 ... -5.98789901  1.29177074\n","  2.31293913] (12665,)\n","y hat: [0.99207627 0.96751712 0.90438554 ... 0.00250265 0.78444675 0.909943  ]\n","Loss at step 689: -4.247233192751367\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 689: 0.9973154362416108\n","Performing step 690 of gradient descent.\n","z [ 4.83184122  3.39521456  2.24793476 ... -5.99050105  1.29246526\n","  2.31402912] (12665,)\n","y hat: [0.99209122 0.96755464 0.90447224 ... 0.00249616 0.78456417 0.91003228]\n","Loss at step 690: -4.242890347302828\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 690: 0.9973154362416108\n","Performing step 691 of gradient descent.\n","z [ 4.83374154  3.39640739  2.24893657 ... -5.99309973  1.29315901\n","  2.31511784] (12665,)\n","y hat: [0.99210611 0.96759207 0.90455877 ... 0.0024897  0.7846814  0.91012138]\n","Loss at step 691: -4.238559060015404\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 691: 0.9973154362416108\n","Performing step 692 of gradient descent.\n","z [ 4.83563923  3.39759845  2.24993712 ... -5.99569504  1.293852\n","  2.3162053 ] (12665,)\n","y hat: [0.99212096 0.9676294  0.90464511 ... 0.00248326 0.78479847 0.91021029]\n","Loss at step 692: -4.234239283140608\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 692: 0.9973154362416108\n","Performing step 693 of gradient descent.\n","z [ 4.83753429  3.39878772  2.25093641 ... -5.998287    1.29454423\n","  2.31729149] (12665,)\n","y hat: [0.99213576 0.96766663 0.90473128 ... 0.00247685 0.78491535 0.91029902]\n","Loss at step 693: -4.229930969196223\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 693: 0.9973154362416108\n","Performing step 694 of gradient descent.\n","z [ 4.83942675  3.39997523  2.25193444 ... -6.00087562  1.29523571\n","  2.31837642] (12665,)\n","y hat: [0.99215051 0.96770376 0.90481727 ... 0.00247046 0.78503207 0.91038757]\n","Loss at step 694: -4.225634070964372\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 694: 0.9973154362416108\n","Performing step 695 of gradient descent.\n","z [ 4.84131659  3.40116097  2.25293122 ... -6.00346089  1.29592643\n","  2.3194601 ] (12665,)\n","y hat: [0.99216522 0.9677408  0.90490308 ... 0.0024641  0.78514861 0.91047594]\n","Loss at step 695: -4.221348541489889\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 695: 0.9973154362416108\n","Performing step 696 of gradient descent.\n","z [ 4.84320383  3.40234495  2.25392675 ... -6.00604285  1.2966164\n","  2.32054253] (12665,)\n","y hat: [0.99217987 0.96777774 0.90498871 ... 0.00245776 0.78526498 0.91056413]\n","Loss at step 696: -4.217074334078226\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 696: 0.9973154362416108\n","Performing step 697 of gradient descent.\n","z [ 4.84508849  3.40352718  2.25492103 ... -6.00862148  1.29730562\n","  2.3216237 ] (12665,)\n","y hat: [0.99219448 0.96781459 0.90507417 ... 0.00245145 0.78538117 0.91065214]\n","Loss at step 697: -4.21281140229376\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 697: 0.9973154362416108\n","Performing step 698 of gradient descent.\n","z [ 4.84697055  3.40470766  2.25591407 ... -6.01119681  1.29799409\n","  2.32270363] (12665,)\n","y hat: [0.99220905 0.96785134 0.90515945 ... 0.00244516 0.7854972  0.91073997]\n","Loss at step 698: -4.208559699958068\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 698: 0.9973154362416108\n","Performing step 699 of gradient descent.\n","z [ 4.84885004  3.4058864   2.25690588 ... -6.01376883  1.29868181\n","  2.32378232] (12665,)\n","y hat: [0.99222356 0.96788799 0.90524456 ... 0.00243889 0.78561305 0.91082762]\n","Loss at step 699: -4.2043191811479215\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 699: 0.9973154362416108\n","Performing step 700 of gradient descent.\n","z [ 4.85072695  3.4070634   2.25789645 ... -6.01633756  1.29936879\n","  2.32485977] (12665,)\n","y hat: [0.99223803 0.96792456 0.90532949 ... 0.00243265 0.78572873 0.91091509]\n","Loss at step 700: -4.200089800193776\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 700: 0.9973154362416108\n","Performing step 701 of gradient descent.\n","z [ 4.85260131  3.40823866  2.25888578 ... -6.01890301  1.30005503\n","  2.32593598] (12665,)\n","y hat: [0.99225245 0.96796102 0.90541425 ... 0.00242643 0.78584424 0.91100239]\n","Loss at step 701: -4.195871511677803\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 701: 0.9973154362416108\n","Performing step 702 of gradient descent.\n","z [ 4.8544731   3.40941219  2.25987389 ... -6.02146519  1.30074053\n","  2.32701096] (12665,)\n","y hat: [0.99226683 0.9679974  0.90549884 ... 0.00242024 0.78595959 0.91108951]\n","Loss at step 702: -4.191664270432152\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 702: 0.9973154362416108\n","Performing step 703 of gradient descent.\n","z [ 4.85634235  3.410584    2.26086077 ... -6.0240241   1.30142529\n","  2.32808472] (12665,)\n","y hat: [0.99228116 0.96803368 0.90558325 ... 0.00241407 0.78607476 0.91117645]\n","Loss at step 703: -4.187468031537399\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 703: 0.9973154362416108\n","Performing step 704 of gradient descent.\n","z [ 4.85820905  3.41175409  2.26184643 ... -6.02657975  1.30210931\n","  2.32915724] (12665,)\n","y hat: [0.99229544 0.96806987 0.9056675  ... 0.00240792 0.78618976 0.91126321]\n","Loss at step 704: -4.183282750320688\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 704: 0.9973154362416108\n","Performing step 705 of gradient descent.\n","z [ 4.86007322  3.41292247  2.26283088 ... -6.02913215  1.3027926\n","  2.33022855] (12665,)\n","y hat: [0.99230968 0.96810596 0.90575157 ... 0.0024018  0.7863046  0.9113498 ]\n","Loss at step 705: -4.179108382353979\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 705: 0.9973154362416108\n","Performing step 706 of gradient descent.\n","z [ 4.86193486  3.41408913  2.2638141  ... -6.03168132  1.30347516\n","  2.33129863] (12665,)\n","y hat: [0.99232388 0.96814197 0.90583547 ... 0.0023957  0.78641927 0.91143622]\n","Loss at step 706: -4.174944883452511\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 706: 0.9973154362416108\n","Performing step 707 of gradient descent.\n","z [ 4.86379398  3.41525409  2.26479612 ... -6.03422725  1.30415699\n","  2.3323675 ] (12665,)\n","y hat: [0.99233802 0.96817788 0.9059192  ... 0.00238962 0.78653377 0.91152246]\n","Loss at step 707: -4.170792209673058\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 707: 0.9973154362416108\n","Performing step 708 of gradient descent.\n","z [ 4.86565058  3.41641736  2.26577692 ... -6.03676996  1.30483809\n","  2.33343516] (12665,)\n","y hat: [0.99235213 0.9682137  0.90600276 ... 0.00238357 0.7866481  0.91160853]\n","Loss at step 708: -4.166650317312217\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 708: 0.9973154362416108\n","Performing step 709 of gradient descent.\n","z [ 4.86750468  3.41757892  2.26675652 ... -6.03930945  1.30551847\n","  2.33450161] (12665,)\n","y hat: [0.99236619 0.96824943 0.90608615 ... 0.00237754 0.78676227 0.91169442]\n","Loss at step 709: -4.162519162904858\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 709: 0.9973154362416108\n","Performing step 710 of gradient descent.\n","z [ 4.86935628  3.4187388   2.26773492 ... -6.04184573  1.30619812\n","  2.33556685] (12665,)\n","y hat: [0.9923802  0.96828506 0.90616937 ... 0.00237153 0.78687627 0.91178015]\n","Loss at step 710: -4.1583987032224385\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 710: 0.9973154362416108\n","Performing step 711 of gradient descent.\n","z [ 4.87120538  3.41989699  2.26871212 ... -6.04437882  1.30687705\n","  2.33663089] (12665,)\n","y hat: [0.99239417 0.96832061 0.90625243 ... 0.00236554 0.7869901  0.9118657 ]\n","Loss at step 711: -4.15428889527132\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 711: 0.9973154362416108\n","Performing step 712 of gradient descent.\n","z [ 4.873052    3.4210535   2.26968812 ... -6.04690872  1.30755526\n","  2.33769372] (12665,)\n","y hat: [0.9924081  0.96835607 0.90633532 ... 0.00235958 0.78710378 0.91195108]\n","Loss at step 712: -4.150189696291359\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 712: 0.9973154362416108\n","Performing step 713 of gradient descent.\n","z [ 4.87489614  3.42220834  2.27066293 ... -6.04943543  1.30823276\n","  2.33875537] (12665,)\n","y hat: [0.99242198 0.96839144 0.90641804 ... 0.00235364 0.78721728 0.91203628]\n","Loss at step 713: -4.146101063754099\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 713: 0.9973154362416108\n","Performing step 714 of gradient descent.\n","z [ 4.8767378   3.42336151  2.27163656 ... -6.05195897  1.30890953\n","  2.33981581] (12665,)\n","y hat: [0.99243582 0.96842672 0.90650059 ... 0.00234772 0.78733062 0.91212132]\n","Loss at step 714: -4.142022955361273\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 714: 0.9973154362416108\n","Performing step 715 of gradient descent.\n","z [ 4.878577    3.424513    2.27260899 ... -6.05447934  1.3095856\n","  2.34087507] (12665,)\n","y hat: [0.99244961 0.96846191 0.90658298 ... 0.00234183 0.7874438  0.91220619]\n","Loss at step 715: -4.137955329043302\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 715: 0.9973154362416108\n","Performing step 716 of gradient descent.\n","z [ 4.88041374  3.42566284  2.27358024 ... -6.05699656  1.31026095\n","  2.34193314] (12665,)\n","y hat: [0.99246336 0.96849701 0.9066652  ... 0.00233595 0.78755682 0.91229089]\n","Loss at step 716: -4.133898142957668\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 716: 0.9973154362416108\n","Performing step 717 of gradient descent.\n","z [ 4.88224803  3.42681102  2.27455031 ... -6.05951063  1.31093559\n","  2.34299003] (12665,)\n","y hat: [0.99247707 0.96853202 0.90674726 ... 0.0023301  0.78766967 0.91237542]\n","Loss at step 717: -4.129851355487304\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 717: 0.9973154362416108\n","Performing step 718 of gradient descent.\n","z [ 4.88407988  3.42795755  2.27551921 ... -6.06202156  1.31160953\n","  2.34404574] (12665,)\n","y hat: [0.99249073 0.96856694 0.90682915 ... 0.00232427 0.78778236 0.91245979]\n","Loss at step 718: -4.125814925239304\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 718: 0.9973154362416108\n","Performing step 719 of gradient descent.\n","z [ 4.88590928  3.42910243  2.27648693 ... -6.06452935  1.31228275\n","  2.34510027] (12665,)\n","y hat: [0.99250436 0.96860178 0.90691088 ... 0.00231846 0.78789489 0.91254398]\n","Loss at step 719: -4.121788811043148\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 719: 0.9973154362416108\n","Performing step 720 of gradient descent.\n","z [ 4.88773625  3.43024567  2.27745348 ... -6.06703402  1.31295528\n","  2.34615362] (12665,)\n","y hat: [0.99251793 0.96863653 0.90699245 ... 0.00231268 0.78800726 0.91262801]\n","Loss at step 720: -4.11777297194929\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 720: 0.9973154362416108\n","Performing step 721 of gradient descent.\n","z [ 4.8895608   3.43138727  2.27841886 ... -6.06953557  1.3136271\n","  2.3472058 ] (12665,)\n","y hat: [0.99253147 0.9686712  0.90707386 ... 0.00230691 0.78811947 0.91271187]\n","Loss at step 721: -4.113767367227746\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 721: 0.9973154362416108\n","Performing step 722 of gradient descent.\n","z [ 4.89138292  3.43252723  2.27938308 ... -6.07203401  1.31429822\n","  2.34825682] (12665,)\n","y hat: [0.99254497 0.96870577 0.9071551  ... 0.00230117 0.78823151 0.91279557]\n","Loss at step 722: -4.10977195636652\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 722: 0.9973154362416108\n","Performing step 723 of gradient descent.\n","z [ 4.89320264  3.43366557  2.28034614 ... -6.07452935  1.31496864\n","  2.34930666] (12665,)\n","y hat: [0.99255842 0.96874026 0.90723618 ... 0.00229545 0.7883434  0.9128791 ]\n","Loss at step 723: -4.105786699070172\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 723: 0.9973154362416108\n","Performing step 724 of gradient descent.\n","z [ 4.89501995  3.43480228  2.28130804 ... -6.0770216   1.31563837\n","  2.35035535] (12665,)\n","y hat: [0.99257183 0.96877467 0.9073171  ... 0.00228975 0.78845513 0.91296247]\n","Loss at step 724: -4.101811555258329\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 724: 0.9973154362416108\n","Performing step 725 of gradient descent.\n","z [ 4.89683486  3.43593737  2.28226878 ... -6.07951076  1.3163074\n","  2.35140288] (12665,)\n","y hat: [0.9925852  0.96880898 0.90739786 ... 0.00228407 0.7885667  0.91304567]\n","Loss at step 725: -4.09784648506433\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 725: 0.9973154362416108\n","Performing step 726 of gradient descent.\n","z [ 4.89864738  3.43707085  2.28322838 ... -6.08199685  1.31697574\n","  2.35244925] (12665,)\n","y hat: [0.99259853 0.96884322 0.90747846 ... 0.00227841 0.78867811 0.91312871]\n","Loss at step 726: -4.093891448833658\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 726: 0.9973154362416108\n","Performing step 727 of gradient descent.\n","z [ 4.90045751  3.43820271  2.28418682 ... -6.08447986  1.31764338\n","  2.35349446] (12665,)\n","y hat: [0.99261181 0.96887737 0.9075589  ... 0.00227277 0.78878936 0.91321159]\n","Loss at step 727: -4.08994640712266\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 727: 0.9973154362416108\n","Performing step 728 of gradient descent.\n","z [ 4.90226526  3.43933297  2.28514412 ... -6.08695981  1.31831034\n","  2.35453853] (12665,)\n","y hat: [0.99262506 0.96891143 0.90763918 ... 0.00226715 0.78890045 0.9132943 ]\n","Loss at step 728: -4.086011320696993\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 728: 0.9973154362416108\n","Performing step 729 of gradient descent.\n","z [ 4.90407063  3.44046163  2.28610027 ... -6.08943671  1.31897661\n","  2.35558144] (12665,)\n","y hat: [0.99263826 0.96894541 0.90771931 ... 0.00226156 0.78901139 0.91337685]\n","Loss at step 729: -4.082086150530402\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 729: 0.9973154362416108\n","Performing step 730 of gradient descent.\n","z [ 4.90587365  3.44158869  2.28705528 ... -6.09191056  1.3196422\n","  2.35662322] (12665,)\n","y hat: [0.99265143 0.9689793  0.90779927 ... 0.00225598 0.78912217 0.91345924]\n","Loss at step 730: -4.078170857803239\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 730: 0.9973154362416108\n","Performing step 731 of gradient descent.\n","z [ 4.9076743   3.44271415  2.28800916 ... -6.09438137  1.3203071\n","  2.35766385] (12665,)\n","y hat: [0.99266455 0.96901312 0.90787908 ... 0.00225043 0.7892328  0.91354147]\n","Loss at step 731: -4.074265403900929\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 731: 0.9973154362416108\n","Performing step 732 of gradient descent.\n","z [ 4.90947259  3.44383803  2.2889619  ... -6.09684915  1.32097132\n","  2.35870334] (12665,)\n","y hat: [0.99267763 0.96904685 0.90795873 ... 0.00224489 0.78934326 0.91362353]\n","Loss at step 732: -4.070369750412957\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 732: 0.9973154362416108\n","Performing step 733 of gradient descent.\n","z [ 4.91126854  3.44496032  2.28991351 ... -6.09931391  1.32163486\n","  2.3597417 ] (12665,)\n","y hat: [0.99269068 0.96908049 0.90803823 ... 0.00223938 0.78945358 0.91370544]\n","Loss at step 733: -4.066483859131222\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 733: 0.9973154362416108\n","Performing step 734 of gradient descent.\n","z [ 4.91306215  3.44608103  2.29086399 ... -6.10177565  1.32229772\n","  2.36077893] (12665,)\n","y hat: [0.99270368 0.96911405 0.90811757 ... 0.00223389 0.78956373 0.91378719]\n","Loss at step 734: -4.062607692048807\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 734: 0.9973154362416108\n","Performing step 735 of gradient descent.\n","z [ 4.91485342  3.44720016  2.29181335 ... -6.10423438  1.32295991\n","  2.36181502] (12665,)\n","y hat: [0.99271664 0.96914753 0.90819675 ... 0.00222841 0.78967374 0.91386878]\n","Loss at step 735: -4.058741211358649\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 735: 0.9973154362416108\n","Performing step 736 of gradient descent.\n","z [ 4.91664236  3.44831772  2.29276158 ... -6.10669011  1.32362142\n","  2.36284999] (12665,)\n","y hat: [0.99272957 0.96918093 0.90827578 ... 0.00222296 0.78978358 0.91395021]\n","Loss at step 736: -4.054884379452176\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 736: 0.9973154362416108\n","Performing step 737 of gradient descent.\n","z [ 4.91842898  3.44943372  2.2937087  ... -6.10914285  1.32428225\n","  2.36388383] (12665,)\n","y hat: [0.99274245 0.96921425 0.90835465 ... 0.00221753 0.78989328 0.91403148]\n","Loss at step 737: -4.051037158918033\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 737: 0.9973154362416108\n","Performing step 738 of gradient descent.\n","z [ 4.92021328  3.45054815  2.2946547  ... -6.1115926   1.32494242\n","  2.36491655] (12665,)\n","y hat: [0.99275529 0.96924748 0.90843337 ... 0.00221211 0.79000282 0.91411259]\n","Loss at step 738: -4.04719951254079\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 738: 0.9973154362416108\n","Performing step 739 of gradient descent.\n","z [ 4.92199527  3.45166102  2.29559958 ... -6.11403937  1.32560192\n","  2.36594816] (12665,)\n","y hat: [0.9927681  0.96928064 0.90851194 ... 0.00220672 0.79011221 0.91419355]\n","Loss at step 739: -4.043371403299654\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 739: 0.9973154362416108\n","Performing step 740 of gradient descent.\n","z [ 4.92377496  3.45277234  2.29654336 ... -6.11648318  1.32626075\n","  2.36697865] (12665,)\n","y hat: [0.99278087 0.96931371 0.90859036 ... 0.00220134 0.79022145 0.91427435]\n","Loss at step 740: -4.039552794367097\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 740: 0.9973154362416108\n","Performing step 741 of gradient descent.\n","z [ 4.92555235  3.45388211  2.29748602 ... -6.11892401  1.32691891\n","  2.36800802] (12665,)\n","y hat: [0.99279359 0.9693467  0.90866862 ... 0.00219599 0.79033053 0.914355  ]\n","Loss at step 741: -4.035743649107737\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 741: 0.9973154362416108\n","Performing step 742 of gradient descent.\n","z [ 4.92732745  3.45499033  2.29842758 ... -6.1213619   1.32757641\n","  2.36903629] (12665,)\n","y hat: [0.99280628 0.96937962 0.90874673 ... 0.00219065 0.79043946 0.91443549]\n","Loss at step 742: -4.031943931076889\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 742: 0.9973154362416108\n","Performing step 743 of gradient descent.\n","z [ 4.92910026  3.45609702  2.29936804 ... -6.12379683  1.32823325\n","  2.37006345] (12665,)\n","y hat: [0.99281893 0.96941245 0.90882469 ... 0.00218534 0.79054824 0.91451582]\n","Loss at step 743: -4.028153604019551\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 743: 0.9973154362416108\n","Performing step 744 of gradient descent.\n","z [ 4.93087079  3.45720216  2.3003074  ... -6.12622882  1.32888943\n","  2.3710895 ] (12665,)\n","y hat: [0.99283154 0.9694452  0.90890249 ... 0.00218004 0.79065687 0.914596  ]\n","Loss at step 744: -4.024372631868985\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 744: 0.9973943939992104\n","Performing step 745 of gradient descent.\n","z [ 4.93263905  3.45830577  2.30124566 ... -6.12865788  1.32954494\n","  2.37211445] (12665,)\n","y hat: [0.99284412 0.96947787 0.90898015 ... 0.00217476 0.79076535 0.91467603]\n","Loss at step 745: -4.020600978745562\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 745: 0.9973943939992104\n","Performing step 746 of gradient descent.\n","z [ 4.93440504  3.45940785  2.30218283 ... -6.13108401  1.33019981\n","  2.37313831] (12665,)\n","y hat: [0.99285665 0.96951047 0.90905766 ... 0.00216951 0.79087368 0.9147559 ]\n","Loss at step 746: -4.016838608955478\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 746: 0.9973943939992104\n","Performing step 747 of gradient descent.\n","z [ 4.93616876  3.4605084   2.30311891 ... -6.13350721  1.33085401\n","  2.37416106] (12665,)\n","y hat: [0.99286915 0.96954298 0.90913502 ... 0.00216427 0.79098186 0.91483562]\n","Loss at step 747: -4.013085486989631\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 747: 0.9973943939992104\n","Performing step 748 of gradient descent.\n","z [ 4.93793024  3.46160744  2.30405389 ... -6.13592751  1.33150756\n","  2.37518273] (12665,)\n","y hat: [0.99288161 0.96957542 0.90921222 ... 0.00215905 0.79108989 0.91491518]\n","Loss at step 748: -4.009341577522289\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 748: 0.9973943939992104\n","Performing step 749 of gradient descent.\n","z [ 4.93968946  3.46270495  2.3049878  ... -6.1383449   1.33216046\n","  2.3762033 ] (12665,)\n","y hat: [0.99289404 0.96960778 0.90928928 ... 0.00215384 0.79119778 0.91499459]\n","Loss at step 749: -4.005606845410157\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 749: 0.9973943939992104\n","Performing step 750 of gradient descent.\n","z [ 4.94144643  3.46380095  2.30592062 ... -6.1407594   1.33281271\n","  2.37722279] (12665,)\n","y hat: [0.99290642 0.96964006 0.9093662  ... 0.00214866 0.79130551 0.91507386]\n","Loss at step 750: -4.001881255690825\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 750: 0.9973943939992104\n","Performing step 751 of gradient descent.\n","z [ 4.94320117  3.46489545  2.30685236 ... -6.143171    1.33346431\n","  2.37824119] (12665,)\n","y hat: [0.99291877 0.96967226 0.90944296 ... 0.0021435  0.7914131  0.91515297]\n","Loss at step 751: -3.998164773581963\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 751: 0.9973943939992104\n","Performing step 752 of gradient descent.\n","z [ 4.94495368  3.46598844  2.30778302 ... -6.14557972  1.33411527\n","  2.37925851] (12665,)\n","y hat: [0.99293108 0.96970439 0.90951958 ... 0.00213835 0.79152053 0.91523193]\n","Loss at step 752: -3.994457364479797\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 752: 0.9973943939992104\n","Performing step 753 of gradient descent.\n","z [ 4.94670396  3.46707993  2.30871261 ... -6.14798556  1.33476558\n","  2.38027475] (12665,)\n","y hat: [0.99294336 0.96973644 0.90959605 ... 0.00213322 0.79162782 0.91531073]\n","Loss at step 753: -3.990758993958353\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 753: 0.9973943939992104\n","Performing step 754 of gradient descent.\n","z [ 4.94845202  3.46816992  2.30964112 ... -6.15038853  1.33541524\n","  2.38128991] (12665,)\n","y hat: [0.99295559 0.96976841 0.90967237 ... 0.00212811 0.79173497 0.91538939]\n","Loss at step 754: -3.987069627767963\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 754: 0.9973943939992104\n","Performing step 755 of gradient descent.\n","z [ 4.95019786  3.46925842  2.31056857 ... -6.15278863  1.33606426\n","  2.382304  ] (12665,)\n","y hat: [0.99296779 0.96980031 0.90974855 ... 0.00212302 0.79184197 0.9154679 ]\n","Loss at step 755: -3.9833892318342876\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 755: 0.9973943939992104\n","Performing step 756 of gradient descent.\n","z [ 4.9519415   3.47034543  2.31149495 ... -6.15518588  1.33671265\n","  2.38331702] (12665,)\n","y hat: [0.99297996 0.96983213 0.90982458 ... 0.00211795 0.79194882 0.91554626]\n","Loss at step 756: -3.9797177722571595\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 756: 0.9973943939992104\n","Performing step 757 of gradient descent.\n","z [ 4.95368293  3.47143095  2.31242027 ... -6.15758029  1.33736039\n","  2.38432897] (12665,)\n","y hat: [0.99299209 0.96986387 0.90990047 ... 0.0021129  0.79205552 0.91562448]\n","Loss at step 757: -3.976055215309565\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 757: 0.9973943939992104\n","Performing step 758 of gradient descent.\n","z [ 4.95542216  3.472515    2.31334452 ... -6.15997185  1.3380075\n","  2.38533985] (12665,)\n","y hat: [0.99300418 0.96989554 0.90997621 ... 0.00210786 0.79216208 0.91570254]\n","Loss at step 758: -3.9724015274362765\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 758: 0.9973943939992104\n","Performing step 759 of gradient descent.\n","z [ 4.9571592   3.47359756  2.31426772 ... -6.16236057  1.33865397\n","  2.38634967] (12665,)\n","y hat: [0.99301624 0.96992713 0.91005181 ... 0.00210284 0.7922685  0.91578046]\n","Loss at step 759: -3.968756675253024\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 759: 0.9973943939992104\n","Performing step 760 of gradient descent.\n","z [ 4.95889406  3.47467866  2.31518986 ... -6.16474647  1.33929981\n","  2.38735842] (12665,)\n","y hat: [0.99302826 0.96995865 0.91012727 ... 0.00209784 0.79237477 0.91585823]\n","Loss at step 760: -3.9651206255452336\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 760: 0.9973943939992104\n","Performing step 761 of gradient descent.\n","z [ 4.96062673  3.47575828  2.31611095 ... -6.16712954  1.33994501\n","  2.38836612] (12665,)\n","y hat: [0.99304024 0.96999009 0.91020258 ... 0.00209286 0.7924809  0.91593585]\n","Loss at step 761: -3.9614933452670695\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 761: 0.9973943939992104\n","Performing step 762 of gradient descent.\n","z [ 4.96235723  3.47683644  2.31703099 ... -6.1695098   1.34058959\n","  2.38937277] (12665,)\n","y hat: [0.99305219 0.97002146 0.91027775 ... 0.00208789 0.79258688 0.91601333]\n","Loss at step 762: -3.9578748015401435\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 762: 0.9973943939992104\n","Performing step 763 of gradient descent.\n","z [ 4.96408556  3.47791314  2.31794998 ... -6.17188726  1.34123353\n","  2.39037836] (12665,)\n","y hat: [0.99306411 0.97005276 0.91035278 ... 0.00208295 0.79269272 0.91609066]\n","Loss at step 763: -3.9542649616528305\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 763: 0.9973943939992104\n","Performing step 764 of gradient descent.\n","z [ 4.96581173  3.47898838  2.31886793 ... -6.17426191  1.34187685\n","  2.3913829 ] (12665,)\n","y hat: [0.99307599 0.97008398 0.91042766 ... 0.00207802 0.79279842 0.91616784]\n","Loss at step 764: -3.95066379305867\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 764: 0.9973943939992104\n","Performing step 765 of gradient descent.\n","z [ 4.96753574  3.48006217  2.31978483 ... -6.17663376  1.34251954\n","  2.39238639] (12665,)\n","y hat: [0.99308783 0.97011512 0.91050241 ... 0.0020731  0.79290397 0.91624488]\n","Loss at step 765: -3.94707126337577\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 765: 0.9973943939992104\n","Performing step 766 of gradient descent.\n","z [ 4.96925759  3.4811345   2.32070069 ... -6.17900283  1.34316161\n","  2.39338884] (12665,)\n","y hat: [0.99309964 0.9701462  0.91057701 ... 0.00206821 0.79300939 0.91632178]\n","Loss at step 766: -3.9434873403856354\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 766: 0.9973943939992104\n","Performing step 767 of gradient descent.\n","z [ 4.9709773   3.4822054   2.32161551 ... -6.18136912  1.34380306\n","  2.39439024] (12665,)\n","y hat: [0.99311142 0.9701772  0.91065147 ... 0.00206333 0.79311466 0.91639853]\n","Loss at step 767: -3.9399119920320294\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 767: 0.9973943939992104\n","Performing step 768 of gradient descent.\n","z [ 4.97269487  3.48327484  2.3225293  ... -6.18373264  1.34444388\n","  2.3953906 ] (12665,)\n","y hat: [0.99312316 0.97020812 0.9107258  ... 0.00205847 0.79321979 0.91647514]\n","Loss at step 768: -3.936345186420003\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 768: 0.9973943939992104\n","Performing step 769 of gradient descent.\n","z [ 4.9744103   3.48434285  2.32344206 ... -6.18609338  1.34508409\n","  2.39638993] (12665,)\n","y hat: [0.99313486 0.97023898 0.91079998 ... 0.00205362 0.79332477 0.9165516 ]\n","Loss at step 769: -3.9327868918148696\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 769: 0.9973943939992104\n","Performing step 770 of gradient descent.\n","z [ 4.9761236   3.48540943  2.32435379 ... -6.18845137  1.34572368\n","  2.39738822] (12665,)\n","y hat: [0.99314653 0.97026976 0.91087403 ... 0.0020488  0.79342962 0.91662793]\n","Loss at step 770: -3.9292370766412823\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 770: 0.9973943939992104\n","Performing step 771 of gradient descent.\n","z [ 4.97783477  3.48647457  2.32526449 ... -6.1908066   1.34636265\n","  2.39838548] (12665,)\n","y hat: [0.99315817 0.97030047 0.91094793 ... 0.00204399 0.79353433 0.91670411]\n","Loss at step 771: -3.925695709482117\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 771: 0.9973943939992104\n","Performing step 772 of gradient descent.\n","z [ 4.97954383  3.48753829  2.32617417 ... -6.19315909  1.347001\n","  2.39938171] (12665,)\n","y hat: [0.99316977 0.97033111 0.9110217  ... 0.00203919 0.7936389  0.91678014]\n","Loss at step 772: -3.922162759077478\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 772: 0.9973943939992104\n","Performing step 773 of gradient descent.\n","z [ 4.98125076  3.48860058  2.32708282 ... -6.19550883  1.34763875\n","  2.40037691] (12665,)\n","y hat: [0.99318134 0.97036168 0.91109533 ... 0.00203442 0.79374332 0.91685604]\n","Loss at step 773: -3.918638194323741\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 773: 0.9973943939992104\n","Performing step 774 of gradient descent.\n","z [ 4.9829556   3.48966146  2.32799045 ... -6.19785584  1.34827588\n","  2.40137108] (12665,)\n","y hat: [0.99319288 0.97039217 0.91116882 ... 0.00202966 0.79384761 0.9169318 ]\n","Loss at step 774: -3.915121984272647\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 774: 0.9973943939992104\n","Performing step 775 of gradient descent.\n","z [ 4.98465832  3.49072092  2.32889707 ... -6.20020012  1.3489124\n","  2.40236424] (12665,)\n","y hat: [0.99320438 0.97042259 0.91124217 ... 0.00202492 0.79395176 0.91700741]\n","Loss at step 775: -3.911614098130152\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 775: 0.9973943939992104\n","Performing step 776 of gradient descent.\n","z [ 4.98635895  3.49177896  2.32980267 ... -6.20254168  1.34954831\n","  2.40335637] (12665,)\n","y hat: [0.99321585 0.97045295 0.91131539 ... 0.00202019 0.79405577 0.91708289]\n","Loss at step 776: -3.9081145052556034\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 776: 0.9973943939992104\n","Performing step 777 of gradient descent.\n","z [ 4.98805749  3.4928356   2.33070726 ... -6.20488052  1.35018362\n","  2.40434748] (12665,)\n","y hat: [0.99322729 0.97048323 0.91138847 ... 0.00201548 0.79415965 0.91715822]\n","Loss at step 777: -3.90462317516075\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 777: 0.9973943939992104\n","Performing step 778 of gradient descent.\n","z [ 4.98975393  3.49389083  2.33161084 ... -6.20721666  1.35081832\n","  2.40533758] (12665,)\n","y hat: [0.99323869 0.97051344 0.91146142 ... 0.00201079 0.79426338 0.91723342]\n","Loss at step 778: -3.901140077508755\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 778: 0.9973943939992104\n","Performing step 779 of gradient descent.\n","z [ 4.9914483   3.49494466  2.33251342 ... -6.20955009  1.35145241\n","  2.40632667] (12665,)\n","y hat: [0.99325006 0.97054359 0.91153423 ... 0.00200611 0.79436698 0.91730847]\n","Loss at step 779: -3.8976651821132418\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 779: 0.9973943939992104\n","Performing step 780 of gradient descent.\n","z [ 4.99314059  3.49599709  2.33341498 ... -6.21188083  1.35208591\n","  2.40731475] (12665,)\n","y hat: [0.99326139 0.97057366 0.9116069  ... 0.00200145 0.79447044 0.91738339]\n","Loss at step 780: -3.8941984589374194\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 780: 0.9973943939992104\n","Performing step 781 of gradient descent.\n","z [ 4.9948308   3.49704813  2.33431555 ... -6.21420888  1.3527188\n","  2.40830181] (12665,)\n","y hat: [0.9932727  0.97060366 0.91167944 ... 0.0019968  0.79457376 0.91745817]\n","Loss at step 781: -3.8907398780930746\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 781: 0.9973943939992104\n","Performing step 782 of gradient descent.\n","z [ 4.99651895  3.49809777  2.33521512 ... -6.21653424  1.35335109\n","  2.40928788] (12665,)\n","y hat: [0.99328397 0.9706336  0.91175185 ... 0.00199217 0.79467695 0.91753281]\n","Loss at step 782: -3.8872894098397204\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 782: 0.9973943939992104\n","Performing step 783 of gradient descent.\n","z [ 4.99820504  3.49914603  2.33611369 ... -6.21885693  1.35398279\n","  2.41027294] (12665,)\n","y hat: [0.99329521 0.97066346 0.91182412 ... 0.00198756 0.79478    0.91760732]\n","Loss at step 783: -3.883847024583632\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 783: 0.9973943939992104\n","Performing step 784 of gradient descent.\n","z [ 4.99988907  3.50019291  2.33701126 ... -6.22117695  1.35461388\n","  2.411257  ] (12665,)\n","y hat: [0.99330641 0.97069326 0.91189626 ... 0.00198297 0.79488292 0.91768169]\n","Loss at step 784: -3.880412692876912\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 784: 0.9973943939992104\n","Performing step 785 of gradient descent.\n","z [ 5.00157105  3.50123841  2.33790784 ... -6.22349431  1.35524439\n","  2.41224006] (12665,)\n","y hat: [0.99331759 0.97072299 0.91196827 ... 0.00197838 0.7949857  0.91775592]\n","Loss at step 785: -3.8769863854166733\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 785: 0.9973943939992104\n","Performing step 786 of gradient descent.\n","z [ 5.00325098  3.50228253  2.33880343 ... -6.225809    1.35587429\n","  2.41322212] (12665,)\n","y hat: [0.99332873 0.97075264 0.91204014 ... 0.00197382 0.79508834 0.91783002]\n","Loss at step 786: -3.873568073044066\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 786: 0.9973943939992104\n","Performing step 787 of gradient descent.\n","z [ 5.00492888  3.50332528  2.33969804 ... -6.22812105  1.35650361\n","  2.4142032 ] (12665,)\n","y hat: [0.99333984 0.97078224 0.91211188 ... 0.00196927 0.79519086 0.91790398]\n","Loss at step 787: -3.8701577267433995\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 787: 0.9973943939992104\n","Performing step 788 of gradient descent.\n","z [ 5.00660473  3.50436665  2.34059165 ... -6.23043045  1.35713234\n","  2.41518328] (12665,)\n","y hat: [0.99335091 0.97081176 0.91218349 ... 0.00196474 0.79529323 0.9179778 ]\n","Loss at step 788: -3.8667553176412413\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 788: 0.9973943939992104\n","Performing step 789 of gradient descent.\n","z [ 5.00827856  3.50540667  2.34148429 ... -6.23273721  1.35776047\n","  2.41616237] (12665,)\n","y hat: [0.99336196 0.97084121 0.91225497 ... 0.00196022 0.79539547 0.91805149]\n","Loss at step 789: -3.863360817005658\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 789: 0.9973943939992104\n","Performing step 790 of gradient descent.\n","z [ 5.00995036  3.50644532  2.34237594 ... -6.23504133  1.35838802\n","  2.41714047] (12665,)\n","y hat: [0.99337298 0.9708706  0.91232632 ... 0.00195572 0.79549758 0.91812505]\n","Loss at step 790: -3.8599741962451715\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 790: 0.9973943939992104\n","Performing step 791 of gradient descent.\n","z [ 5.01162014  3.50748261  2.34326662 ... -6.23734283  1.35901498\n","  2.41811759] (12665,)\n","y hat: [0.99338396 0.97089992 0.91239753 ... 0.00195123 0.79559956 0.91819847]\n","Loss at step 791: -3.856595426907926\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 791: 0.9973943939992104\n","Performing step 792 of gradient descent.\n","z [ 5.0132879   3.50851854  2.34415632 ... -6.23964171  1.35964136\n","  2.41909373] (12665,)\n","y hat: [0.99339491 0.97092918 0.91246862 ... 0.00194676 0.7957014  0.91827176]\n","Loss at step 792: -3.8532244806810407\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 792: 0.9973943939992104\n","Performing step 793 of gradient descent.\n","z [ 5.01495365  3.50955312  2.34504505 ... -6.24193798  1.36026715\n","  2.42006889] (12665,)\n","y hat: [0.99340583 0.97095837 0.91253957 ... 0.0019423  0.79580311 0.91834491]\n","Loss at step 793: -3.8498613293895168\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 793: 0.9973943939992104\n","Performing step 794 of gradient descent.\n","z [ 5.0166174   3.51058636  2.3459328  ... -6.24423164  1.36089236\n","  2.42104308] (12665,)\n","y hat: [0.99341672 0.97098749 0.9126104  ... 0.00193786 0.79590469 0.91841793]\n","Loss at step 794: -3.846505944995376\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 794: 0.9973943939992104\n","Performing step 795 of gradient descent.\n","z [ 5.01827915  3.51161825  2.34681959 ... -6.24652269  1.36151699\n","  2.42201628] (12665,)\n","y hat: [0.99342758 0.97101654 0.9126811  ... 0.00193343 0.79600614 0.91849082]\n","Loss at step 795: -3.843158299597047\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 795: 0.9973943939992104\n","Performing step 796 of gradient descent.\n","z [ 5.0199389   3.5126488   2.34770541 ... -6.24881115  1.36214104\n","  2.42298852] (12665,)\n","y hat: [0.99343841 0.97104553 0.91275167 ... 0.00192902 0.79610745 0.91856358]\n","Loss at step 796: -3.8398183654283473\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 796: 0.9973943939992104\n","Performing step 797 of gradient descent.\n","z [ 5.02159666  3.51367801  2.34859027 ... -6.25109702  1.36276451\n","  2.42395979] (12665,)\n","y hat: [0.99344921 0.97107445 0.91282211 ... 0.00192463 0.79620864 0.9186362 ]\n","Loss at step 797: -3.836486114857716\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 797: 0.9973943939992104\n","Performing step 798 of gradient descent.\n","z [ 5.02325243  3.51470588  2.34947416 ... -6.2533803   1.36338741\n","  2.42493008] (12665,)\n","y hat: [0.99345997 0.97110331 0.91289242 ... 0.00192025 0.79630969 0.9187087 ]\n","Loss at step 798: -3.8331615203873883\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 798: 0.9973943939992104\n","Performing step 799 of gradient descent.\n","z [ 5.02490623  3.51573242  2.35035709 ... -6.255661    1.36400973\n","  2.42589941] (12665,)\n","y hat: [0.99347071 0.9711321  0.91296261 ... 0.00191588 0.79641061 0.91878106]\n","Loss at step 799: -3.8298445546525604\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 799: 0.9973943939992104\n","Performing step 800 of gradient descent.\n","z [ 5.02655804  3.51675764  2.35123907 ... -6.25793914  1.36463148\n","  2.42686778] (12665,)\n","y hat: [0.99348142 0.97116083 0.91303267 ... 0.00191153 0.7965114  0.91885329]\n","Loss at step 800: -3.8265351904206586\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 800: 0.9973943939992104\n","Performing step 801 of gradient descent.\n","z [ 5.02820789  3.51778153  2.35212009 ... -6.2602147   1.36525265\n","  2.42783519] (12665,)\n","y hat: [0.99349209 0.9711895  0.9131026  ... 0.00190719 0.79661207 0.9189254 ]\n","Loss at step 801: -3.823233400590405\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 801: 0.9973943939992104\n","Performing step 802 of gradient descent.\n","z [ 5.02985577  3.5188041   2.35300016 ... -6.26248771  1.36587326\n","  2.42880164] (12665,)\n","y hat: [0.99350274 0.97121809 0.9131724  ... 0.00190287 0.7967126  0.91899737]\n","Loss at step 802: -3.819939158191161\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 802: 0.9973943939992104\n","Performing step 803 of gradient descent.\n","z [ 5.03150168  3.51982535  2.35387928 ... -6.26475816  1.36649329\n","  2.42976713] (12665,)\n","y hat: [0.99351335 0.97124663 0.91324208 ... 0.00189856 0.796813   0.91906921]\n","Loss at step 803: -3.8166524363819967\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 803: 0.9973943939992104\n","Performing step 804 of gradient descent.\n","z [ 5.03314564  3.52084528  2.35475745 ... -6.26702606  1.36711276\n","  2.43073167] (12665,)\n","y hat: [0.99352394 0.9712751  0.91331163 ... 0.00189427 0.79691328 0.91914093]\n","Loss at step 804: -3.8133732084510497\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 804: 0.9973943939992104\n","Performing step 805 of gradient descent.\n","z [ 5.03478765  3.52186391  2.35563468 ... -6.26929142  1.36773166\n","  2.43169525] (12665,)\n","y hat: [0.99353449 0.9713035  0.91338106 ... 0.00188999 0.79701342 0.91921251]\n","Loss at step 805: -3.810101447814598\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 805: 0.9973943939992104\n","Performing step 806 of gradient descent.\n","z [ 5.03642771  3.52288122  2.35651096 ... -6.27155424  1.36834999\n","  2.43265789] (12665,)\n","y hat: [0.99354502 0.97133184 0.91345036 ... 0.00188573 0.79711344 0.91928397]\n","Loss at step 806: -3.8068371280164204\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 806: 0.9973943939992104\n","Performing step 807 of gradient descent.\n","z [ 5.03806583  3.52389723  2.35738629 ... -6.27381453  1.36896776\n","  2.43361957] (12665,)\n","y hat: [0.99355552 0.97136012 0.91351954 ... 0.00188148 0.79721333 0.9193553 ]\n","Loss at step 807: -3.803580222726981\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 807: 0.9973943939992104\n","Performing step 808 of gradient descent.\n","z [ 5.03970201  3.52491194  2.35826069 ... -6.2760723   1.36958496\n","  2.43458032] (12665,)\n","y hat: [0.99356599 0.97138834 0.9135886  ... 0.00187724 0.79731309 0.9194265 ]\n","Loss at step 808: -3.800330705742571\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 808: 0.9973943939992104\n","Performing step 809 of gradient descent.\n","z [ 5.04133626  3.52592535  2.35913415 ... -6.27832754  1.37020161\n","  2.43554011] (12665,)\n","y hat: [0.99357643 0.97141649 0.91365753 ... 0.00187302 0.79741272 0.91949758]\n","Loss at step 809: -3.7970885509846557\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 809: 0.9973943939992104\n","Performing step 810 of gradient descent.\n","z [ 5.04296858  3.52693746  2.36000668 ... -6.28058027  1.37081769\n","  2.43649897] (12665,)\n","y hat: [0.99358684 0.97144458 0.91372633 ... 0.00186881 0.79751223 0.91956852]\n","Loss at step 810: -3.7938537324991763\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 810: 0.9973943939992104\n","Performing step 811 of gradient descent.\n","z [ 5.04459897  3.52794829  2.36087828 ... -6.2828305   1.37143322\n","  2.43745689] (12665,)\n","y hat: [0.99359722 0.97147261 0.91379502 ... 0.00186462 0.79761161 0.91963935]\n","Loss at step 811: -3.790626224455657\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 811: 0.9973943939992104\n","Performing step 812 of gradient descent.\n","z [ 5.04622745  3.52895782  2.36174894 ... -6.28507822  1.37204819\n","  2.43841387] (12665,)\n","y hat: [0.99360757 0.97150057 0.91386358 ... 0.00186044 0.79771087 0.91971004]\n","Loss at step 812: -3.787406001146552\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 812: 0.9973943939992104\n","Performing step 813 of gradient descent.\n","z [ 5.04785401  3.52996607  2.36261867 ... -6.28732344  1.3726626\n","  2.43936992] (12665,)\n","y hat: [0.99361789 0.97152847 0.91393201 ... 0.00185628 0.79781    0.91978061]\n","Loss at step 813: -3.784193036986501\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 813: 0.9973943939992104\n","Performing step 814 of gradient descent.\n","z [ 5.04947867  3.53097303  2.36348748 ... -6.28956617  1.37327646\n","  2.44032503] (12665,)\n","y hat: [0.99362818 0.97155631 0.91400033 ... 0.00185213 0.797909   0.91985105]\n","Loss at step 814: -3.7809873065115824\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 814: 0.9973943939992104\n","Performing step 815 of gradient descent.\n","z [ 5.05110141  3.53197872  2.36435537 ... -6.29180642  1.37388976\n","  2.44127921] (12665,)\n","y hat: [0.99363845 0.97158409 0.91406853 ... 0.00184799 0.79800788 0.91992137]\n","Loss at step 815: -3.7777887843786213\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 815: 0.9973943939992104\n","Performing step 816 of gradient descent.\n","z [ 5.05272226  3.53298313  2.36522233 ... -6.29404419  1.37450251\n","  2.44223247] (12665,)\n","y hat: [0.99364869 0.97161181 0.9141366  ... 0.00184387 0.79810663 0.91999157]\n","Loss at step 816: -3.774597445364348\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 816: 0.9973943939992104\n","Performing step 817 of gradient descent.\n","z [ 5.05434121  3.53398627  2.36608837 ... -6.29627948  1.37511471\n","  2.4431848 ] (12665,)\n","y hat: [0.9936589  0.97163947 0.91420455 ... 0.00183976 0.79820526 0.92006164]\n","Loss at step 817: -3.7714132643649023\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 817: 0.9973943939992104\n","Performing step 818 of gradient descent.\n","z [ 5.05595827  3.53498813  2.3669535  ... -6.29851231  1.37572636\n","  2.4441362 ] (12665,)\n","y hat: [0.99366908 0.97166706 0.91427238 ... 0.00183566 0.79830376 0.92013158]\n","Loss at step 818: -3.768236216394942\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 818: 0.9973943939992104\n","Performing step 819 of gradient descent.\n","z [ 5.05757344  3.53598873  2.36781771 ... -6.30074267  1.37633747\n","  2.44508669] (12665,)\n","y hat: [0.99367923 0.97169459 0.91434009 ... 0.00183158 0.79840214 0.92020141]\n","Loss at step 819: -3.7650662765869933\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 819: 0.9973943939992104\n","Performing step 820 of gradient descent.\n","z [ 5.05918673  3.53698807  2.36868101 ... -6.30297057  1.37694802\n","  2.44603625] (12665,)\n","y hat: [0.99368935 0.97172207 0.91440768 ... 0.00182751 0.79850039 0.92027111]\n","Loss at step 820: -3.7619034201907326\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 820: 0.9973943939992104\n","Performing step 821 of gradient descent.\n","z [ 5.06079814  3.53798614  2.36954339 ... -6.30519602  1.37755803\n","  2.4469849 ] (12665,)\n","y hat: [0.99369945 0.97174948 0.91447516 ... 0.00182346 0.79859852 0.92034068]\n","Loss at step 821: -3.758747622572402\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 821: 0.9973943939992104\n","Performing step 822 of gradient descent.\n","z [ 5.06240768  3.53898296  2.37040487 ... -6.30741903  1.3781675\n","  2.44793263] (12665,)\n","y hat: [0.99370952 0.97177683 0.91454251 ... 0.00181942 0.79869653 0.92041014]\n","Loss at step 822: -3.7555988592139444\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 822: 0.9973943939992104\n","Performing step 823 of gradient descent.\n","z [ 5.06401535  3.53997853  2.37126544 ... -6.30963959  1.37877642\n","  2.44887944] (12665,)\n","y hat: [0.99371956 0.97180412 0.91460974 ... 0.00181539 0.79879442 0.92047947]\n","Loss at step 823: -3.7524571057124376\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 823: 0.9973943939992104\n","Performing step 824 of gradient descent.\n","z [ 5.06562116  3.54097284  2.3721251  ... -6.31185772  1.3793848\n","  2.44982535] (12665,)\n","y hat: [0.99372958 0.97183136 0.91467686 ... 0.00181137 0.79889218 0.92054868]\n","Loss at step 824: -3.7493223377794034\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 824: 0.9973943939992104\n","Performing step 825 of gradient descent.\n","z [ 5.0672251   3.5419659   2.37298386 ... -6.31407342  1.37999264\n","  2.45077034] (12665,)\n","y hat: [0.99373956 0.97185853 0.91474385 ... 0.00180737 0.79898982 0.92061777]\n","Loss at step 825: -3.7461945312400746\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 825: 0.9973943939992104\n","Performing step 826 of gradient descent.\n","z [ 5.06882719  3.54295772  2.37384172 ... -6.3162867   1.38059994\n","  2.45171443] (12665,)\n","y hat: [0.99374952 0.97188564 0.91481073 ... 0.00180338 0.79908734 0.92068673]\n","Loss at step 826: -3.743073662032756\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 826: 0.9973943939992104\n","Performing step 827 of gradient descent.\n","z [ 5.07042743  3.5439483   2.37469869 ... -6.31849755  1.3812067\n","  2.45265762] (12665,)\n","y hat: [0.99375945 0.9719127  0.91487749 ... 0.00179941 0.79918473 0.92075558]\n","Loss at step 827: -3.7399597062082206\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 827: 0.9973943939992104\n","Performing step 828 of gradient descent.\n","z [ 5.07202582  3.54493764  2.37555475 ... -6.32070599  1.38181292\n","  2.4535999 ] (12665,)\n","y hat: [0.99376936 0.97193969 0.91494414 ... 0.00179544 0.79928201 0.92082431]\n","Loss at step 828: -3.736852639928969\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 828: 0.9973943939992104\n","Performing step 829 of gradient descent.\n","z [ 5.07362237  3.54592574  2.37640992 ... -6.32291203  1.38241861\n","  2.45454128] (12665,)\n","y hat: [0.99377924 0.97196663 0.91501066 ... 0.00179149 0.79937916 0.92089291]\n","Loss at step 829: -3.733752439468555\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 829: 0.9973943939992104\n","Performing step 830 of gradient descent.\n","z [ 5.07521708  3.54691261  2.3772642  ... -6.32511566  1.38302377\n","  2.45548176] (12665,)\n","y hat: [0.99378909 0.9719935  0.91507707 ... 0.00178756 0.79947619 0.9209614 ]\n","Loss at step 830: -3.7306590812110314\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 830: 0.9973943939992104\n","Performing step 831 of gradient descent.\n","z [ 5.07680996  3.54789825  2.37811759 ... -6.32731689  1.38362839\n","  2.45642134] (12665,)\n","y hat: [0.99379891 0.97202032 0.91514337 ... 0.00178363 0.7995731  0.92102976]\n","Loss at step 831: -3.7275725416502707\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 831: 0.9973943939992104\n","Performing step 832 of gradient descent.\n","z [ 5.07840101  3.54888266  2.37897009 ... -6.32951573  1.38423248\n","  2.45736003] (12665,)\n","y hat: [0.99380871 0.97204708 0.91520955 ... 0.00177972 0.7996699  0.92109801]\n","Loss at step 832: -3.724492797389189\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 832: 0.9973943939992104\n","Performing step 833 of gradient descent.\n","z [ 5.07999024  3.54986584  2.3798217  ... -6.33171219  1.38483604\n","  2.45829782] (12665,)\n","y hat: [0.99381848 0.97207378 0.91527561 ... 0.00177583 0.79976657 0.92116614]\n","Loss at step 833: -3.7214198251393964\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 833: 0.9973943939992104\n","Performing step 834 of gradient descent.\n","z [ 5.08157764  3.55084781  2.38067243 ... -6.33390626  1.38543907\n","  2.45923472] (12665,)\n","y hat: [0.99382822 0.97210043 0.91534156 ... 0.00177194 0.79986312 0.92123415]\n","Loss at step 834: -3.7183536017201546\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 834: 0.9973943939992104\n","Performing step 835 of gradient descent.\n","z [ 5.08316323  3.55182856  2.38152228 ... -6.33609796  1.38604157\n","  2.46017074] (12665,)\n","y hat: [0.99383794 0.97212702 0.91540739 ... 0.00176807 0.79995955 0.92130204]\n","Loss at step 835: -3.7152941040582212\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 835: 0.9973943939992104\n","Performing step 836 of gradient descent.\n","z [ 5.084747    3.55280809  2.38237124 ... -6.33828728  1.38664355\n","  2.46110587] (12665,)\n","y hat: [0.99384763 0.97215354 0.91547311 ... 0.00176421 0.80005586 0.92136982]\n","Loss at step 836: -3.7122413091868514\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 836: 0.9973943939992104\n","Performing step 837 of gradient descent.\n","z [ 5.08632897  3.55378641  2.38321933 ... -6.34047424  1.38724499\n","  2.46204011] (12665,)\n","y hat: [0.9938573  0.97218002 0.91553871 ... 0.00176036 0.80015206 0.92143747]\n","Loss at step 837: -3.7091951942453156\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 837: 0.9973943939992104\n","Performing step 838 of gradient descent.\n","z [ 5.08790913  3.55476352  2.38406654 ... -6.34265884  1.38784592\n","  2.46297347] (12665,)\n","y hat: [0.99386694 0.97220643 0.9156042  ... 0.00175653 0.80024813 0.92150501]\n","Loss at step 838: -3.7061557364783515\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 838: 0.9973943939992104\n","Performing step 839 of gradient descent.\n","z [ 5.0894875   3.55573942  2.38491288 ... -6.34484108  1.38844632\n","  2.46390595] (12665,)\n","y hat: [0.99387655 0.97223279 0.91566958 ... 0.0017527  0.80034409 0.92157244]\n","Loss at step 839: -3.70312291323535\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 839: 0.9973943939992104\n","Performing step 840 of gradient descent.\n","z [ 5.09106407  3.55671412  2.38575834 ... -6.34702098  1.3890462\n","  2.46483755] (12665,)\n","y hat: [0.99388614 0.97225909 0.91573484 ... 0.00174889 0.80043993 0.92163974]\n","Loss at step 840: -3.7000967019700717\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 840: 0.9973943939992104\n","Performing step 841 of gradient descent.\n","z [ 5.09263884  3.55768762  2.38660294 ... -6.34919852  1.38964556\n","  2.46576827] (12665,)\n","y hat: [0.9938957  0.97228534 0.91579999 ... 0.0017451  0.80053565 0.92170693]\n","Loss at step 841: -3.6970770802396764\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 841: 0.9973943939992104\n","Performing step 842 of gradient descent.\n","z [ 5.09421184  3.55865992  2.38744666 ... -6.35137373  1.39024439\n","  2.46669811] (12665,)\n","y hat: [0.99390524 0.97231152 0.91586503 ... 0.00174131 0.80063126 0.92177401]\n","Loss at step 842: -3.6940640257043853\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 842: 0.9973943939992104\n","Performing step 843 of gradient descent.\n","z [ 5.09578305  3.55963103  2.38828952 ... -6.35354661  1.39084271\n","  2.46762709] (12665,)\n","y hat: [0.99391475 0.97233766 0.91592995 ... 0.00173754 0.80072674 0.92184097]\n","Loss at step 843: -3.691057516126765\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 843: 0.9973943939992104\n","Performing step 844 of gradient descent.\n","z [ 5.09735248  3.56060095  2.38913152 ... -6.35571715  1.39144051\n","  2.46855519] (12665,)\n","y hat: [0.99392423 0.97236373 0.91599476 ... 0.00173378 0.80082211 0.92190781]\n","Loss at step 844: -3.6880575293712488\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 844: 0.9973943939992104\n","Performing step 845 of gradient descent.\n","z [ 5.09892013  3.56156967  2.38997265 ... -6.35788537  1.3920378\n","  2.46948242] (12665,)\n","y hat: [0.99393369 0.97238975 0.91605946 ... 0.00173003 0.80091737 0.92197454]\n","Loss at step 845: -3.6850640434033775\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 845: 0.9973943939992104\n","Performing step 846 of gradient descent.\n","z [ 5.10048602  3.56253721  2.39081292 ... -6.36005127  1.39263457\n","  2.47040879] (12665,)\n","y hat: [0.99394313 0.97241572 0.91612405 ... 0.00172629 0.8010125  0.92204115]\n","Loss at step 846: -3.682077036289371\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 846: 0.9973943939992104\n","Performing step 847 of gradient descent.\n","z [ 5.10205014  3.56350356  2.39165233 ... -6.36221486  1.39323083\n","  2.47133429] (12665,)\n","y hat: [0.99395253 0.97244162 0.91618853 ... 0.00172257 0.80110753 0.92210765]\n","Loss at step 847: -3.6790964861954953\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 847: 0.9973943939992104\n","Performing step 848 of gradient descent.\n","z [ 5.1036125   3.56446874  2.39249089 ... -6.36437613  1.39382657\n","  2.47225892] (12665,)\n","y hat: [0.99396192 0.97246748 0.9162529  ... 0.00171886 0.80120243 0.92217404]\n","Loss at step 848: -3.6761223713874593\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 848: 0.9973943939992104\n","Performing step 849 of gradient descent.\n","z [ 5.1051731   3.56543273  2.39332859 ... -6.3665351   1.3944218\n","  2.4731827 ] (12665,)\n","y hat: [0.99397128 0.97249328 0.91631716 ... 0.00171516 0.80129722 0.92224031]\n","Loss at step 849: -3.6731546702299016\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 849: 0.9973943939992104\n","Performing step 850 of gradient descent.\n","z [ 5.10673195  3.56639555  2.39416544 ... -6.36869178  1.39501652\n","  2.47410561] (12665,)\n","y hat: [0.99398061 0.97251902 0.91638131 ... 0.00171147 0.8013919  0.92230647]\n","Loss at step 850: -3.6701933611857562\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 850: 0.9973943939992104\n","Performing step 851 of gradient descent.\n","z [ 5.10828904  3.5673572   2.39500143 ... -6.37084616  1.39561074\n","  2.47502767] (12665,)\n","y hat: [0.99398992 0.97254471 0.91644534 ... 0.00170779 0.80148645 0.92237252]\n","Loss at step 851: -3.6672384228157657\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 851: 0.9973943939992104\n","Performing step 852 of gradient descent.\n","z [ 5.1098444   3.56831768  2.39583658 ... -6.37299825  1.39620444\n","  2.47594887] (12665,)\n","y hat: [0.9939992  0.97257034 0.91650927 ... 0.00170412 0.8015809  0.92243845]\n","Loss at step 852: -3.6642898337779326\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 852: 0.9973943939992104\n","Performing step 853 of gradient descent.\n","z [ 5.11139801  3.56927699  2.39667088 ... -6.37514805  1.39679764\n","  2.47686921] (12665,)\n","y hat: [0.99400846 0.97259593 0.91657309 ... 0.00170047 0.80167523 0.92250427]\n","Loss at step 853: -3.6613475728267915\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 853: 0.9973943939992104\n","Performing step 854 of gradient descent.\n","z [ 5.11294988  3.57023514  2.39750434 ... -6.37729558  1.39739033\n","  2.4777887 ] (12665,)\n","y hat: [0.9940177  0.97262145 0.9166368  ... 0.00169683 0.80176945 0.92256998]\n","Loss at step 854: -3.6584116188130604\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 854: 0.9973943939992104\n","Performing step 855 of gradient descent.\n","z [ 5.11450002  3.57119212  2.39833695 ... -6.37944083  1.39798252\n","  2.47870735] (12665,)\n","y hat: [0.99402691 0.97264692 0.9167004  ... 0.0016932  0.80186355 0.92263558]\n","Loss at step 855: -3.655481950683039\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 855: 0.9974733517568101\n","Performing step 856 of gradient descent.\n","z [ 5.11604843  3.57214795  2.39916872 ... -6.38158381  1.3985742\n","  2.47962514] (12665,)\n","y hat: [0.9940361  0.97267234 0.91676389 ... 0.00168958 0.80195754 0.92270107]\n","Loss at step 856: -3.6525585474780273\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 856: 0.9974733517568101\n","Performing step 857 of gradient descent.\n","z [ 5.11759511  3.57310262  2.39999965 ... -6.38372453  1.39916538\n","  2.48054209] (12665,)\n","y hat: [0.99404526 0.97269771 0.91682728 ... 0.00168597 0.80205141 0.92276644]\n","Loss at step 857: -3.6496413883337837\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 857: 0.9974733517568101\n","Performing step 858 of gradient descent.\n","z [ 5.11914007  3.57405614  2.40082974 ... -6.38586299  1.39975606\n","  2.48145819] (12665,)\n","y hat: [0.9940544  0.97272302 0.91689055 ... 0.00168238 0.80214518 0.9228317 ]\n","Loss at step 858: -3.646730452480026\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 858: 0.9974733517568101\n","Performing step 859 of gradient descent.\n","z [ 5.12068332  3.5750085   2.401659   ... -6.38799919  1.40034624\n","  2.48237345] (12665,)\n","y hat: [0.99406351 0.97274827 0.91695372 ... 0.00167879 0.80223883 0.92289686]\n","Loss at step 859: -3.6438257192398606\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 859: 0.9974733517568101\n","Performing step 860 of gradient descent.\n","z [ 5.12222485  3.57595972  2.40248742 ... -6.39013315  1.40093592\n","  2.48328787] (12665,)\n","y hat: [0.9940726  0.97277348 0.91701679 ... 0.00167522 0.80233236 0.9229619 ]\n","Loss at step 860: -3.640927168029346\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 860: 0.9974733517568101\n","Performing step 861 of gradient descent.\n","z [ 5.12376467  3.5769098   2.40331501 ... -6.39226486  1.4015251\n","  2.48420145] (12665,)\n","y hat: [0.99408167 0.97279863 0.91707974 ... 0.00167166 0.80242579 0.92302683]\n","Loss at step 861: -3.638034778356775\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 861: 0.9974733517568101\n","Performing step 862 of gradient descent.\n","z [ 5.12530278  3.57785873  2.40414177 ... -6.39439432  1.40211379\n","  2.48511419] (12665,)\n","y hat: [0.99409071 0.97282373 0.91714259 ... 0.00166811 0.8025191  0.92309166]\n","Loss at step 862: -3.635148529822385\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 862: 0.9974733517568101\n","Performing step 863 of gradient descent.\n","z [ 5.12683919  3.57880652  2.4049677  ... -6.39652156  1.40270198\n","  2.48602609] (12665,)\n","y hat: [0.99409973 0.97284878 0.91720533 ... 0.00166457 0.8026123  0.92315637]\n","Loss at step 863: -3.632268402117672\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 863: 0.9974733517568101\n","Performing step 864 of gradient descent.\n","z [ 5.12837391  3.57975318  2.4057928  ... -6.39864656  1.40328967\n","  2.48693716] (12665,)\n","y hat: [0.99410872 0.97287377 0.91726797 ... 0.00166104 0.80270539 0.92322098]\n","Loss at step 864: -3.629394375025021\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 864: 0.9974733517568101\n","Performing step 865 of gradient descent.\n","z [ 5.12990693  3.5806987   2.40661708 ... -6.40076934  1.40387687\n","  2.4878474 ] (12665,)\n","y hat: [0.9941177  0.97289871 0.9173305  ... 0.00165753 0.80279837 0.92328547]\n","Loss at step 865: -3.6265264284169567\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 865: 0.9974733517568101\n","Performing step 866 of gradient descent.\n","z [ 5.13143826  3.58164308  2.40744054 ... -6.40288989  1.40446358\n","  2.48875681] (12665,)\n","y hat: [0.99412664 0.9729236  0.91739292 ... 0.00165402 0.80289124 0.92334986]\n","Loss at step 866: -3.623664542255929\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 866: 0.9974733517568101\n","Performing step 867 of gradient descent.\n","z [ 5.1329679   3.58258634  2.40826317 ... -6.40500823  1.4050498\n","  2.48966539] (12665,)\n","y hat: [0.99413557 0.97294844 0.91745525 ... 0.00165053 0.80298399 0.92341414]\n","Loss at step 867: -3.620808696593591\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 867: 0.9974733517568101\n","Performing step 868 of gradient descent.\n","z [ 5.13449586  3.58352847  2.40908499 ... -6.40712436  1.40563553\n","  2.49057314] (12665,)\n","y hat: [0.99414447 0.97297322 0.91751746 ... 0.00164704 0.80307664 0.92347831]\n","Loss at step 868: -3.6179588715704094\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 868: 0.9974733517568101\n","Performing step 869 of gradient descent.\n","z [ 5.13602214  3.58446948  2.40990598 ... -6.40923828  1.40622076\n","  2.49148007] (12665,)\n","y hat: [0.99415335 0.97299796 0.91757957 ... 0.00164357 0.80316917 0.92354238]\n","Loss at step 869: -3.615115047415132\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 869: 0.9974733517568101\n","Performing step 870 of gradient descent.\n","z [ 5.13754675  3.58540937  2.41072616 ... -6.41135     1.40680551\n","  2.49238618] (12665,)\n","y hat: [0.9941622  0.97302264 0.91764158 ... 0.00164011 0.8032616  0.92360634]\n","Loss at step 870: -3.612277204444308\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 870: 0.9974733517568101\n","Performing step 871 of gradient descent.\n","z [ 5.13906968  3.58634814  2.41154553 ... -6.41345952  1.40738978\n","  2.49329146] (12665,)\n","y hat: [0.99417103 0.97304727 0.91770348 ... 0.00163666 0.80335392 0.92367019]\n","Loss at step 871: -3.6094453230617316\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 871: 0.9974733517568101\n","Performing step 872 of gradient descent.\n","z [ 5.14059095  3.58728579  2.41236409 ... -6.41556685  1.40797355\n","  2.49419592] (12665,)\n","y hat: [0.99417984 0.97307185 0.91776528 ... 0.00163322 0.80344612 0.92373393]\n","Loss at step 872: -3.6066193837580482\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 872: 0.9974733517568101\n","Performing step 873 of gradient descent.\n","z [ 5.14211055  3.58822233  2.41318183 ... -6.41767199  1.40855685\n","  2.49509957] (12665,)\n","y hat: [0.99418863 0.97309638 0.91782698 ... 0.00162979 0.80353822 0.92379757]\n","Loss at step 873: -3.6037993671102315\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 873: 0.9974733517568101\n","Performing step 874 of gradient descent.\n","z [ 5.14362849  3.58915775  2.41399876 ... -6.41977495  1.40913965\n","  2.4960024 ] (12665,)\n","y hat: [0.99419739 0.97312086 0.91788857 ... 0.00162637 0.80363021 0.9238611 ]\n","Loss at step 874: -3.600985253781141\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 874: 0.9974733517568101\n","Performing step 875 of gradient descent.\n","z [ 5.14514478  3.59009207  2.41481489 ... -6.42187573  1.40972198\n","  2.49690441] (12665,)\n","y hat: [0.99420613 0.97314529 0.91795006 ... 0.00162297 0.80372209 0.92392452]\n","Loss at step 875: -3.5981770245188716\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 875: 0.9974733517568101\n","Performing step 876 of gradient descent.\n","z [ 5.14665941  3.59102528  2.41563021 ... -6.42397433  1.41030382\n","  2.49780561] (12665,)\n","y hat: [0.99421485 0.97316966 0.91801145 ... 0.00161957 0.80381386 0.92398784]\n","Loss at step 876: -3.5953746601565792\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 876: 0.9974733517568101\n","Performing step 877 of gradient descent.\n","z [ 5.14817239  3.59195739  2.41644473 ... -6.42607077  1.41088519\n","  2.498706  ] (12665,)\n","y hat: [0.99422355 0.97319399 0.91807273 ... 0.00161618 0.80390552 0.92405106]\n","Loss at step 877: -3.592578141611767\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 877: 0.9974733517568101\n","Performing step 878 of gradient descent.\n","z [ 5.14968373  3.5928884   2.41725845 ... -6.42816503  1.41146607\n","  2.49960559] (12665,)\n","y hat: [0.99423222 0.97321827 0.91813391 ... 0.00161281 0.80399708 0.92411417]\n","Loss at step 878: -3.5897874498858533\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 878: 0.9974733517568101\n","Performing step 879 of gradient descent.\n","z [ 5.15119343  3.59381831  2.41807136 ... -6.43025714  1.41204647\n","  2.50050436] (12665,)\n","y hat: [0.99424087 0.9732425  0.918195   ... 0.00160944 0.80408853 0.92417717]\n","Loss at step 879: -3.587002566063851\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 879: 0.9974733517568101\n","Performing step 880 of gradient descent.\n","z [ 5.15270149  3.59474713  2.41888348 ... -6.4323471   1.4126264\n","  2.50140233] (12665,)\n","y hat: [0.9942495  0.97326667 0.91825598 ... 0.00160609 0.80417987 0.92424007]\n","Loss at step 880: -3.584223471313666\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 880: 0.9973943939992104\n","Performing step 881 of gradient descent.\n","z [ 5.15420791  3.59567485  2.4196948  ... -6.4344349   1.41320585\n","  2.50229949] (12665,)\n","y hat: [0.99425811 0.9732908  0.91831685 ... 0.00160274 0.8042711  0.92430287]\n","Loss at step 881: -3.5814501468859032\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 881: 0.9973943939992104\n","Performing step 882 of gradient descent.\n","z [ 5.15571271  3.59660148  2.42050533 ... -6.43652055  1.41378483\n","  2.50319585] (12665,)\n","y hat: [0.99426669 0.97331488 0.91837763 ... 0.00159941 0.80436222 0.92436556]\n","Loss at step 882: -3.578682574113276\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 882: 0.9973943939992104\n","Performing step 883 of gradient descent.\n","z [ 5.15721588  3.59752702  2.42131506 ... -6.43860407  1.41436333\n","  2.50409141] (12665,)\n","y hat: [0.99427525 0.97333891 0.91843831 ... 0.00159609 0.80445324 0.92442815]\n","Loss at step 883: -3.575920734410022\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 883: 0.9973943939992104\n","Performing step 884 of gradient descent.\n","z [ 5.15871742  3.59845147  2.422124   ... -6.44068544  1.41494136\n","  2.50498617] (12665,)\n","y hat: [0.99428379 0.97336289 0.91849889 ... 0.00159277 0.80454416 0.92449063]\n","Loss at step 884: -3.573164609271733\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 884: 0.9973943939992104\n","Performing step 885 of gradient descent.\n","z [ 5.16021734  3.59937485  2.42293215 ... -6.44276468  1.41551891\n","  2.50588014] (12665,)\n","y hat: [0.99429231 0.97338682 0.91855936 ... 0.00158947 0.80463496 0.92455301]\n","Loss at step 885: -3.570414180274754\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 885: 0.9973943939992104\n","Performing step 886 of gradient descent.\n","z [ 5.16171565  3.60029714  2.42373952 ... -6.44484179  1.416096\n","  2.50677331] (12665,)\n","y hat: [0.99430081 0.9734107  0.91861974 ... 0.00158618 0.80472566 0.92461529]\n","Loss at step 886: -3.5676694290756625\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 886: 0.9973943939992104\n","Performing step 887 of gradient descent.\n","z [ 5.16321235  3.60121835  2.42454609 ... -6.44691678  1.41667261\n","  2.50766568] (12665,)\n","y hat: [0.99430928 0.97343453 0.91868002 ... 0.00158289 0.80481626 0.92467747]\n","Loss at step 887: -3.564930337411045\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 887: 0.9973943939992104\n","Performing step 888 of gradient descent.\n","z [ 5.16470743  3.60213849  2.42535189 ... -6.44898965  1.41724876\n","  2.50855726] (12665,)\n","y hat: [0.99431774 0.97345831 0.91874019 ... 0.00157962 0.80490675 0.92473954]\n","Loss at step 888: -3.5621968870967917\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 888: 0.9973943939992104\n","Performing step 889 of gradient descent.\n","z [ 5.16620091  3.60305755  2.42615689 ... -6.4510604   1.41782443\n","  2.50944805] (12665,)\n","y hat: [0.99432617 0.97348205 0.91880027 ... 0.00157636 0.80499713 0.92480152]\n","Loss at step 889: -3.5594690600279337\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 889: 0.9973943939992104\n","Performing step 890 of gradient descent.\n","z [ 5.16769279  3.60397554  2.42696112 ... -6.45312904  1.41839964\n","  2.51033806] (12665,)\n","y hat: [0.99433458 0.97350574 0.91886025 ... 0.00157311 0.80508741 0.92486339]\n","Loss at step 890: -3.5567468381779865\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 890: 0.9973943939992104\n","Performing step 891 of gradient descent.\n","z [ 5.16918307  3.60489247  2.42776457 ... -6.45519558  1.41897438\n","  2.51122727] (12665,)\n","y hat: [0.99434297 0.97352938 0.91892014 ... 0.00156986 0.80517758 0.92492516]\n","Loss at step 891: -3.5540302035986584\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 891: 0.9973943939992104\n","Performing step 892 of gradient descent.\n","z [ 5.17067176  3.60580832  2.42856724 ... -6.45726001  1.41954866\n","  2.5121157 ] (12665,)\n","y hat: [0.99435134 0.97355297 0.91897992 ... 0.00156663 0.80526765 0.92498682]\n","Loss at step 892: -3.551319138419421\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 892: 0.9973943939992104\n","Performing step 893 of gradient descent.\n","z [ 5.17215885  3.60672312  2.42936913 ... -6.45932234  1.42012247\n","  2.51300335] (12665,)\n","y hat: [0.99435968 0.97357651 0.9190396  ... 0.00156341 0.80535762 0.92504839]\n","Loss at step 893: -3.548613624846927\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 893: 0.9973943939992104\n","Performing step 894 of gradient descent.\n","z [ 5.17364436  3.60763685  2.43017025 ... -6.46138258  1.42069582\n","  2.51389021] (12665,)\n","y hat: [0.99436801 0.97360001 0.91909919 ... 0.0015602  0.80544748 0.92510986]\n","Loss at step 894: -3.545913645164813\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 894: 0.9973943939992104\n","Performing step 895 of gradient descent.\n","z [ 5.17512828  3.60854952  2.43097059 ... -6.46344073  1.42126871\n","  2.51477629] (12665,)\n","y hat: [0.99437631 0.97362346 0.91915868 ... 0.00155699 0.80553723 0.92517122]\n","Loss at step 895: -3.543219181733122\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 895: 0.9973943939992104\n","Performing step 896 of gradient descent.\n","z [ 5.17661062  3.60946114  2.43177017 ... -6.4654968   1.42184114\n","  2.51566159] (12665,)\n","y hat: [0.9943846  0.97364686 0.91921808 ... 0.0015538  0.80562689 0.92523249]\n","Loss at step 896: -3.5405302169879738\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 896: 0.9973943939992104\n","Performing step 897 of gradient descent.\n","z [ 5.17809138  3.6103717   2.43256897 ... -6.46755079  1.4224131\n","  2.51654612] (12665,)\n","y hat: [0.99439286 0.97367021 0.91927737 ... 0.00155062 0.80571644 0.92529365]\n","Loss at step 897: -3.5378467334411337\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 897: 0.9973943939992104\n","Performing step 898 of gradient descent.\n","z [ 5.17957057  3.61128121  2.433367   ... -6.46960269  1.42298461\n","  2.51742987] (12665,)\n","y hat: [0.9944011  0.97369352 0.91933657 ... 0.00154744 0.80580588 0.92535472]\n","Loss at step 898: -3.535168713679466\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 898: 0.9973943939992104\n","Performing step 899 of gradient descent.\n","z [ 5.18104819  3.61218967  2.43416427 ... -6.47165253  1.42355566\n","  2.51831284] (12665,)\n","y hat: [0.99440932 0.97371678 0.91939568 ... 0.00154428 0.80589523 0.92541569]\n","Loss at step 899: -3.5324961403648576\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 899: 0.9973943939992104\n","Performing step 900 of gradient descent.\n","z [ 5.18252425  3.61309708  2.43496077 ... -6.4737003   1.42412625\n","  2.51919504] (12665,)\n","y hat: [0.99441752 0.97373999 0.91945468 ... 0.00154112 0.80598447 0.92547656]\n","Loss at step 900: -3.529828996233451\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 900: 0.9973943939992104\n","Performing step 901 of gradient descent.\n","z [ 5.18399873  3.61400345  2.43575651 ... -6.475746    1.42469638\n","  2.52007647] (12665,)\n","y hat: [0.9944257  0.97376316 0.91951359 ... 0.00153798 0.80607361 0.92553733]\n","Loss at step 901: -3.5271672640953775\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 901: 0.9973943939992104\n","Performing step 902 of gradient descent.\n","z [ 5.18547166  3.61490877  2.43655149 ... -6.47778965  1.42526606\n","  2.52095713] (12665,)\n","y hat: [0.99443386 0.97378628 0.91957241 ... 0.00153485 0.80616264 0.925598  ]\n","Loss at step 902: -3.5245109268345636\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 902: 0.9973943939992104\n","Performing step 903 of gradient descent.\n","z [ 5.18694303  3.61581306  2.43734571 ... -6.47983124  1.42583528\n","  2.52183703] (12665,)\n","y hat: [0.994442   0.97380935 0.91963113 ... 0.00153172 0.80625158 0.92565857]\n","Loss at step 903: -3.521859967407898\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 903: 0.9973943939992104\n","Performing step 904 of gradient descent.\n","z [ 5.18841285  3.61671631  2.43813917 ... -6.48187078  1.42640405\n","  2.52271615] (12665,)\n","y hat: [0.99445012 0.97383238 0.91968975 ... 0.0015286  0.80634041 0.92571904]\n","Loss at step 904: -3.5192143688452977\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 904: 0.9973943939992104\n","Performing step 905 of gradient descent.\n","z [ 5.18988112  3.61761852  2.43893187 ... -6.48390827  1.42697237\n","  2.52359452] (12665,)\n","y hat: [0.99445821 0.97385536 0.91974828 ... 0.0015255  0.80642914 0.92577942]\n","Loss at step 905: -3.5165741142490194\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 905: 0.9973943939992104\n","Performing step 906 of gradient descent.\n","z [ 5.19134784  3.6185197   2.43972382 ... -6.48594373  1.42754024\n","  2.52447211] (12665,)\n","y hat: [0.99446629 0.97387829 0.91980672 ... 0.0015224  0.80651777 0.9258397 ]\n","Loss at step 906: -3.5139391867933174\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 906: 0.9973943939992104\n","Performing step 907 of gradient descent.\n","z [ 5.19281302  3.61941984  2.44051501 ... -6.48797714  1.42810766\n","  2.52534895] (12665,)\n","y hat: [0.99447435 0.97390118 0.91986506 ... 0.00151931 0.8066063  0.92589988]\n","Loss at step 907: -3.511309569724173\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 907: 0.9973943939992104\n","Performing step 908 of gradient descent.\n","z [ 5.19427665  3.62031896  2.44130545 ... -6.49000852  1.42867462\n","  2.52622503] (12665,)\n","y hat: [0.99448238 0.97392403 0.91992331 ... 0.00151623 0.80669472 0.92595996]\n","Loss at step 908: -3.508685246358818\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 908: 0.9973943939992104\n","Performing step 909 of gradient descent.\n","z [ 5.19573875  3.62121705  2.44209514 ... -6.49203787  1.42924114\n","  2.52710035] (12665,)\n","y hat: [0.9944904  0.97394682 0.91998146 ... 0.00151316 0.80678305 0.92601995]\n","Loss at step 909: -3.5060662000853244\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 909: 0.9973943939992104\n","Performing step 910 of gradient descent.\n","z [ 5.19719932  3.62211412  2.44288408 ... -6.49406519  1.42980721\n","  2.52797491] (12665,)\n","y hat: [0.9944984  0.97396958 0.92003952 ... 0.0015101  0.80687127 0.92607984]\n","Loss at step 910: -3.503452414362324\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 910: 0.9973943939992104\n","Performing step 911 of gradient descent.\n","z [ 5.19865836  3.62301017  2.44367227 ... -6.49609049  1.43037283\n","  2.52884871] (12665,)\n","y hat: [0.99450638 0.97399229 0.92009748 ... 0.00150705 0.8069594  0.92613964]\n","Loss at step 911: -3.500843872718546\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 911: 0.9973943939992104\n","Performing step 912 of gradient descent.\n","z [ 5.20011587  3.62390519  2.44445972 ... -6.49811377  1.43093801\n","  2.52972176] (12665,)\n","y hat: [0.99451433 0.97401495 0.92015536 ... 0.00150401 0.80704743 0.92619934]\n","Loss at step 912: -3.498240558752518\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 912: 0.9973943939992104\n","Performing step 913 of gradient descent.\n","z [ 5.20157186  3.6247992   2.44524642 ... -6.50013504  1.43150274\n","  2.53059406] (12665,)\n","y hat: [0.99452227 0.97403757 0.92021314 ... 0.00150098 0.80713535 0.92625894]\n","Loss at step 913: -3.495642456132066\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 913: 0.9973943939992104\n","Performing step 914 of gradient descent.\n","z [ 5.20302632  3.62569219  2.44603238 ... -6.5021543   1.43206703\n","  2.53146561] (12665,)\n","y hat: [0.99453019 0.97406014 0.92027082 ... 0.00149796 0.80722318 0.92631845]\n","Loss at step 914: -3.4930495485941555\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 914: 0.9973943939992104\n","Performing step 915 of gradient descent.\n","z [ 5.20447927  3.62658417  2.4468176  ... -6.50417156  1.43263087\n","  2.53233641] (12665,)\n","y hat: [0.99453809 0.97408267 0.92032842 ... 0.00149494 0.80731091 0.92637786]\n","Loss at step 915: -3.490461819944274\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 915: 0.9973943939992104\n","Performing step 916 of gradient descent.\n","z [ 5.20593071  3.62747514  2.44760208 ... -6.50618681  1.43319427\n","  2.53320646] (12665,)\n","y hat: [0.99454597 0.97410515 0.92038592 ... 0.00149194 0.80739853 0.92643718]\n","Loss at step 916: -3.4878792540562666\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 916: 0.9974733517568101\n","Performing step 917 of gradient descent.\n","z [ 5.20738064  3.62836511  2.44838582 ... -6.50820007  1.43375723\n","  2.53407577] (12665,)\n","y hat: [0.99455382 0.97412759 0.92044333 ... 0.00148894 0.80748606 0.9264964 ]\n","Loss at step 917: -3.4853018348719\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 917: 0.9974733517568101\n","Performing step 918 of gradient descent.\n","z [ 5.20882906  3.62925406  2.44916882 ... -6.51021133  1.43431975\n","  2.53494433] (12665,)\n","y hat: [0.99456166 0.97414998 0.92050065 ... 0.00148595 0.80757349 0.92655553]\n","Loss at step 918: -3.482729546400457\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 918: 0.9974733517568101\n","Performing step 919 of gradient descent.\n","z [ 5.21027597  3.63014201  2.44995109 ... -6.51222061  1.43488184\n","  2.53581215] (12665,)\n","y hat: [0.99456948 0.97417233 0.92055787 ... 0.00148298 0.80766082 0.92661456]\n","Loss at step 919: -3.4801623727184663\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 919: 0.9974733517568101\n","Performing step 920 of gradient descent.\n","z [ 5.21172138  3.63102896  2.45073262 ... -6.5142279   1.43544348\n","  2.53667922] (12665,)\n","y hat: [0.99457729 0.97419464 0.92061501 ... 0.00148001 0.80774806 0.9266735 ]\n","Loss at step 920: -3.477600297969278\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 920: 0.9974733517568101\n","Performing step 921 of gradient descent.\n","z [ 5.2131653   3.63191491  2.45151342 ... -6.5162332   1.43600468\n","  2.53754556] (12665,)\n","y hat: [0.99458507 0.9742169  0.92067205 ... 0.00147705 0.80783519 0.92673235]\n","Loss at step 921: -3.475043306362762\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 921: 0.9974733517568101\n","Performing step 922 of gradient descent.\n","z [ 5.21460772  3.63279986  2.4522935  ... -6.51823654  1.43656545\n","  2.53841116] (12665,)\n","y hat: [0.99459283 0.97423912 0.92072901 ... 0.00147409 0.80792223 0.9267911 ]\n","Loss at step 922: -3.472491382174875\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 922: 0.9974733517568101\n","Performing step 923 of gradient descent.\n","z [ 5.21604865  3.63368382  2.45307284 ... -6.52023789  1.43712578\n","  2.53927602] (12665,)\n","y hat: [0.99460057 0.9742613  0.92078587 ... 0.00147115 0.80800917 0.92684976]\n","Loss at step 923: -3.469944509747471\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 923: 0.9974733517568101\n","Performing step 924 of gradient descent.\n","z [ 5.2174881   3.63456678  2.45385145 ... -6.52223728  1.43768568\n","  2.54014014] (12665,)\n","y hat: [0.9946083  0.97428343 0.92084264 ... 0.00146822 0.80809601 0.92690832]\n","Loss at step 924: -3.4674026734877526\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 924: 0.9974733517568101\n","Performing step 925 of gradient descent.\n","z [ 5.21892606  3.63544876  2.45462934 ... -6.52423471  1.43824514\n","  2.54100353] (12665,)\n","y hat: [0.994616   0.97430552 0.92089933 ... 0.00146529 0.80818275 0.92696679]\n","Loss at step 925: -3.4648658578680505\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 925: 0.9974733517568101\n","Performing step 926 of gradient descent.\n","z [ 5.22036253  3.63632974  2.4554065  ... -6.52623017  1.43880417\n","  2.54186619] (12665,)\n","y hat: [0.99462369 0.97432757 0.92095592 ... 0.00146237 0.8082694  0.92702517]\n","Loss at step 926: -3.4623340474254616\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 926: 0.9974733517568101\n","Performing step 927 of gradient descent.\n","z [ 5.22179753  3.63720974  2.45618294 ... -6.52822367  1.43936277\n","  2.54272811] (12665,)\n","y hat: [0.99463136 0.97434957 0.92101242 ... 0.00145947 0.80835595 0.92708346]\n","Loss at step 927: -3.4598072267615687\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 927: 0.9974733517568101\n","Performing step 928 of gradient descent.\n","z [ 5.22323105  3.63808875  2.45695866 ... -6.53021523  1.43992094\n","  2.54358931] (12665,)\n","y hat: [0.99463901 0.97437153 0.92106884 ... 0.00145657 0.80844241 0.92714166]\n","Loss at step 928: -3.4572853805419386\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 928: 0.9974733517568101\n","Performing step 929 of gradient descent.\n","z [ 5.2246631   3.63896678  2.45773366 ... -6.53220483  1.44047867\n","  2.54444977] (12665,)\n","y hat: [0.99464664 0.97439344 0.92112516 ... 0.00145368 0.80852877 0.92719976]\n","Loss at step 929: -3.4547684934959357\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 929: 0.9974733517568101\n","Performing step 930 of gradient descent.\n","z [ 5.22609368  3.63984383  2.45850794 ... -6.53419249  1.44103598\n","  2.54530951] (12665,)\n","y hat: [0.99465425 0.97441532 0.9211814  ... 0.00145079 0.80861503 0.92725777]\n","Loss at step 930: -3.4522565504163625\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 930: 0.9974733517568101\n","Performing step 931 of gradient descent.\n","z [ 5.2275228   3.6407199   2.4592815  ... -6.5361782   1.44159286\n","  2.54616853] (12665,)\n","y hat: [0.99466185 0.97443715 0.92123755 ... 0.00144792 0.80870119 0.92731569]\n","Loss at step 931: -3.4497495361590107\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 931: 0.9974733517568101\n","Performing step 932 of gradient descent.\n","z [ 5.22895045  3.641595    2.46005435 ... -6.53816198  1.44214931\n","  2.54702682] (12665,)\n","y hat: [0.99466942 0.97445894 0.9212936  ... 0.00144505 0.80878726 0.92737352]\n","Loss at step 932: -3.4472474356425056\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 932: 0.9974733517568101\n","Performing step 933 of gradient descent.\n","z [ 5.23037664  3.64246912  2.46082648 ... -6.54014382  1.44270533\n","  2.54788439] (12665,)\n","y hat: [0.99467698 0.97448069 0.92134957 ... 0.0014422  0.80887324 0.92743126]\n","Loss at step 933: -3.4447502338479565\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 933: 0.9974733517568101\n","Performing step 934 of gradient descent.\n","z [ 5.23180138  3.64334227  2.4615979  ... -6.54212373  1.44326093\n","  2.54874123] (12665,)\n","y hat: [0.99468452 0.97450239 0.92140546 ... 0.00143935 0.80895912 0.9274889 ]\n","Loss at step 934: -3.4422579158184194\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 934: 0.9974733517568101\n","Performing step 935 of gradient descent.\n","z [ 5.23322466  3.64421445  2.4623686  ... -6.54410172  1.4438161\n","  2.54959736] (12665,)\n","y hat: [0.99469204 0.97452405 0.92146125 ... 0.00143651 0.8090449  0.92754646]\n","Loss at step 935: -3.439770466658834\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 935: 0.9974733517568101\n","Performing step 936 of gradient descent.\n","z [ 5.23464649  3.64508567  2.4631386  ... -6.54607778  1.44437084\n","  2.55045277] (12665,)\n","y hat: [0.99469954 0.97454567 0.92151696 ... 0.00143368 0.80913059 0.92760393]\n","Loss at step 936: -3.437287871535594\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 936: 0.9974733517568101\n","Performing step 937 of gradient descent.\n","z [ 5.23606688  3.64595591  2.46390789 ... -6.54805193  1.44492517\n","  2.55130746] (12665,)\n","y hat: [0.99470702 0.97456725 0.92157258 ... 0.00143085 0.80921618 0.9276613 ]\n","Loss at step 937: -3.434810115676171\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 937: 0.9974733517568101\n","Performing step 938 of gradient descent.\n","z [ 5.23748582  3.6468252   2.46467647 ... -6.55002416  1.44547907\n","  2.55216144] (12665,)\n","y hat: [0.99471449 0.97458879 0.92162811 ... 0.00142804 0.80930168 0.92771859]\n","Loss at step 938: -3.4323371843689254\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 938: 0.9974733517568101\n","Performing step 939 of gradient descent.\n","z [ 5.23890331  3.64769352  2.46544435 ... -6.55199448  1.44603255\n","  2.5530147 ] (12665,)\n","y hat: [0.99472193 0.97461029 0.92168356 ... 0.00142523 0.80938709 0.92777578]\n","Loss at step 939: -3.4298690629626845\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 939: 0.9974733517568101\n","Performing step 940 of gradient descent.\n","z [ 5.24031937  3.64856088  2.46621152 ... -6.55396289  1.44658561\n","  2.55386725] (12665,)\n","y hat: [0.99472936 0.97463174 0.92173891 ... 0.00142243 0.8094724  0.92783289]\n","Loss at step 940: -3.4274057368664717\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 940: 0.9974733517568101\n","Performing step 941 of gradient descent.\n","z [ 5.241734    3.64942729  2.46697798 ... -6.5559294   1.44713824\n","  2.55471909] (12665,)\n","y hat: [0.99473677 0.97465315 0.92179419 ... 0.00141964 0.80955762 0.92788991]\n","Loss at step 941: -3.424947191549176\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 941: 0.9974733517568101\n","Performing step 942 of gradient descent.\n","z [ 5.24314719  3.65029274  2.46774375 ... -6.55789401  1.44769046\n","  2.55557022] (12665,)\n","y hat: [0.99474417 0.97467452 0.92184937 ... 0.00141686 0.80964274 0.92794684]\n","Loss at step 942: -3.4224934125393243\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 942: 0.9974733517568101\n","Performing step 943 of gradient descent.\n","z [ 5.24455895  3.65115724  2.46850882 ... -6.55985672  1.44824226\n","  2.55642064] (12665,)\n","y hat: [0.99475154 0.97469585 0.92190447 ... 0.00141409 0.80972777 0.92800368]\n","Loss at step 943: -3.420044385424615\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 943: 0.9974733517568101\n","Performing step 944 of gradient descent.\n","z [ 5.24596928  3.65202079  2.46927319 ... -6.56181754  1.44879365\n","  2.55727036] (12665,)\n","y hat: [0.9947589  0.97471714 0.92195949 ... 0.00141132 0.80981271 0.92806043]\n","Loss at step 944: -3.41760009585179\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 944: 0.9974733517568101\n","Performing step 945 of gradient descent.\n","z [ 5.24737819  3.65288338  2.47003686 ... -6.56377647  1.44934461\n","  2.55811936] (12665,)\n","y hat: [0.99476624 0.97473839 0.92201441 ... 0.00140856 0.80989755 0.92811709]\n","Loss at step 945: -3.415160529526148\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 945: 0.9974733517568101\n","Performing step 946 of gradient descent.\n","z [ 5.24878568  3.65374503  2.47079983 ... -6.56573352  1.44989516\n","  2.55896767] (12665,)\n","y hat: [0.99477356 0.9747596  0.92206926 ... 0.00140581 0.8099823  0.92817367]\n","Loss at step 946: -3.4127256722114416\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 946: 0.9974733517568101\n","Performing step 947 of gradient descent.\n","z [ 5.25019176  3.65460574  2.47156211 ... -6.56768869  1.4504453\n","  2.55981527] (12665,)\n","y hat: [0.99478087 0.97478077 0.92212402 ... 0.00140307 0.81006696 0.92823015]\n","Loss at step 947: -3.410295509729435\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 947: 0.9974733517568101\n","Performing step 948 of gradient descent.\n","z [ 5.25159641  3.6554655   2.4723237  ... -6.56964198  1.45099502\n","  2.56066217] (12665,)\n","y hat: [0.99478816 0.9748019  0.92217869 ... 0.00140034 0.81015152 0.92828655]\n","Loss at step 948: -3.407870027959644\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 948: 0.9974733517568101\n","Performing step 949 of gradient descent.\n","z [ 5.25299966  3.65632432  2.47308459 ... -6.57159339  1.45154433\n","  2.56150837] (12665,)\n","y hat: [0.99479543 0.97482298 0.92223328 ... 0.00139761 0.81023599 0.92834286]\n","Loss at step 949: -3.405449212839017\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 949: 0.9974733517568101\n","Performing step 950 of gradient descent.\n","z [ 5.2544015   3.6571822   2.4738448  ... -6.57354294  1.45209322\n","  2.56235387] (12665,)\n","y hat: [0.99480268 0.97484403 0.92228778 ... 0.00139489 0.81032037 0.92839909]\n","Loss at step 950: -3.4030330503617363\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 950: 0.9974733517568101\n","Performing step 951 of gradient descent.\n","z [ 5.25580193  3.65803915  2.47460431 ... -6.57549061  1.4526417\n","  2.56319867] (12665,)\n","y hat: [0.99480992 0.97486504 0.9223422  ... 0.00139218 0.81040466 0.92845522]\n","Loss at step 951: -3.4006215265788127\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 951: 0.9974733517568101\n","Performing step 952 of gradient descent.\n","z [ 5.25720095  3.65889516  2.47536314 ... -6.57743643  1.45318978\n","  2.56404278] (12665,)\n","y hat: [0.99481714 0.974886   0.92239654 ... 0.00138948 0.81048886 0.92851128]\n","Loss at step 952: -3.3982146275977962\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 952: 0.9974733517568101\n","Performing step 953 of gradient descent.\n","z [ 5.25859858  3.65975024  2.47612128 ... -6.57938038  1.45373744\n","  2.56488619] (12665,)\n","y hat: [0.99482434 0.97490693 0.92245079 ... 0.00138678 0.81057296 0.92856724]\n","Loss at step 953: -3.395812339582627\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 953: 0.9974733517568101\n","Performing step 954 of gradient descent.\n","z [ 5.25999481  3.66060438  2.47687874 ... -6.58132249  1.45428469\n","  2.56572891] (12665,)\n","y hat: [0.99483152 0.97492782 0.92250495 ... 0.0013841  0.81065698 0.92862312]\n","Loss at step 954: -3.3934146487531813\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 954: 0.9974733517568101\n","Performing step 955 of gradient descent.\n","z [ 5.26138965  3.6614576   2.47763551 ... -6.58326273  1.45483154\n","  2.56657094] (12665,)\n","y hat: [0.99483869 0.97494866 0.92255904 ... 0.00138142 0.8107409  0.92867891]\n","Loss at step 955: -3.391021541385073\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 955: 0.9974733517568101\n","Performing step 956 of gradient descent.\n","z [ 5.26278309  3.66230989  2.47839161 ... -6.58520113  1.45537797\n","  2.56741228] (12665,)\n","y hat: [0.99484584 0.97496947 0.92261304 ... 0.00137875 0.81082473 0.92873461]\n","Loss at step 956: -3.3886330038093546\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 956: 0.9974733517568101\n","Performing step 957 of gradient descent.\n","z [ 5.26417515  3.66316125  2.47914702 ... -6.58713769  1.455924\n","  2.56825293] (12665,)\n","y hat: [0.99485297 0.97499024 0.92266696 ... 0.00137608 0.81090847 0.92879023]\n","Loss at step 957: -3.386249022412253\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 957: 0.9974733517568101\n","Performing step 958 of gradient descent.\n","z [ 5.26556582  3.66401169  2.47990175 ... -6.5890724   1.45646963\n","  2.56909289] (12665,)\n","y hat: [0.99486009 0.97501097 0.92272079 ... 0.00137343 0.81099212 0.92884577]\n","Loss at step 958: -3.3838695836348105\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 958: 0.9974733517568101\n","Performing step 959 of gradient descent.\n","z [ 5.26695511  3.66486121  2.4806558  ... -6.59100528  1.45701485\n","  2.56993216] (12665,)\n","y hat: [0.99486719 0.97503166 0.92277454 ... 0.00137078 0.81107568 0.92890122]\n","Loss at step 959: -3.3814946739727154\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 959: 0.9974733517568101\n","Performing step 960 of gradient descent.\n","z [ 5.26834301  3.66570981  2.48140918 ... -6.59293632  1.45755966\n","  2.57077075] (12665,)\n","y hat: [0.99487427 0.97505231 0.92282821 ... 0.00136814 0.81115915 0.92895658]\n","Loss at step 960: -3.3791242799759935\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 960: 0.9974733517568101\n","Performing step 961 of gradient descent.\n","z [ 5.26972954  3.6665575   2.48216188 ... -6.59486553  1.45810407\n","  2.57160866] (12665,)\n","y hat: [0.99488133 0.97507292 0.9228818  ... 0.0013655  0.81124253 0.92901186]\n","Loss at step 961: -3.3767583882486547\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 961: 0.9974733517568101\n","Performing step 962 of gradient descent.\n","z [ 5.2711147   3.66740426  2.48291391 ... -6.59679292  1.45864808\n","  2.57244588] (12665,)\n","y hat: [0.99488838 0.97509349 0.92293531 ... 0.00136288 0.81132582 0.92906705]\n","Loss at step 962: -3.3743969854485427\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 962: 0.9974733517568101\n","Performing step 963 of gradient descent.\n","z [ 5.27249848  3.66825012  2.48366526 ... -6.59871848  1.45919169\n","  2.57328242] (12665,)\n","y hat: [0.99489542 0.97511403 0.92298873 ... 0.00136026 0.81140901 0.92912216]\n","Loss at step 963: -3.372040058286936\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 963: 0.9974733517568101\n","Performing step 964 of gradient descent.\n","z [ 5.27388089  3.66909506  2.48441594 ... -6.60064222  1.45973489\n","  2.57411828] (12665,)\n","y hat: [0.99490243 0.97513452 0.92304207 ... 0.00135765 0.81149212 0.92917719]\n","Loss at step 964: -3.369687593528384\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 964: 0.9974733517568101\n","Performing step 965 of gradient descent.\n","z [ 5.27526194  3.66993909  2.48516596 ... -6.60256415  1.4602777\n","  2.57495346] (12665,)\n","y hat: [0.99490943 0.97515498 0.92309533 ... 0.00135505 0.81157514 0.92923213]\n","Loss at step 965: -3.3673395779904056\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 965: 0.9974733517568101\n","Performing step 966 of gradient descent.\n","z [ 5.27664162  3.67078221  2.4859153  ... -6.60448426  1.4608201\n","  2.57578797] (12665,)\n","y hat: [0.99491641 0.9751754  0.92314851 ... 0.00135245 0.81165807 0.92928699]\n","Loss at step 966: -3.3649959985431543\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 966: 0.9974733517568101\n","Performing step 967 of gradient descent.\n","z [ 5.27801995  3.67162443  2.48666398 ... -6.60640256  1.46136211\n","  2.5766218 ] (12665,)\n","y hat: [0.99492338 0.97519578 0.92320161 ... 0.00134986 0.81174092 0.92934176]\n","Loss at step 967: -3.362656842109294\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 967: 0.9974733517568101\n","Performing step 968 of gradient descent.\n","z [ 5.27939691  3.67246574  2.48741199 ... -6.60831906  1.46190371\n","  2.57745495] (12665,)\n","y hat: [0.99493033 0.97521612 0.92325463 ... 0.00134728 0.81182367 0.92939645]\n","Loss at step 968: -3.3603220956636184\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 968: 0.9974733517568101\n","Performing step 969 of gradient descent.\n","z [ 5.28077253  3.67330615  2.48815933 ... -6.61023376  1.46244492\n","  2.57828743] (12665,)\n","y hat: [0.99493726 0.97523643 0.92330757 ... 0.00134471 0.81190633 0.92945106]\n","Loss at step 969: -3.3579917462328064\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 969: 0.9974733517568101\n","Performing step 970 of gradient descent.\n","z [ 5.28214678  3.67414567  2.48890602 ... -6.61214665  1.46298574\n","  2.57911924] (12665,)\n","y hat: [0.99494418 0.97525669 0.92336042 ... 0.00134214 0.81198891 0.92950558]\n","Loss at step 970: -3.3556657808951766\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 970: 0.9974733517568101\n","Performing step 971 of gradient descent.\n","z [ 5.28351969  3.67498428  2.48965204 ... -6.61405775  1.46352616\n","  2.57995038] (12665,)\n","y hat: [0.99495108 0.97527692 0.9234132  ... 0.00133958 0.8120714  0.92956002]\n","Loss at step 971: -3.353344186780474\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 971: 0.9974733517568101\n","Performing step 972 of gradient descent.\n","z [ 5.28489126  3.67582199  2.49039739 ... -6.61596706  1.46406618\n","  2.58078085] (12665,)\n","y hat: [0.99495797 0.97529711 0.92346589 ... 0.00133703 0.8121538  0.92961438]\n","Loss at step 972: -3.3510269510695423\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 972: 0.9974733517568101\n","Performing step 973 of gradient descent.\n","z [ 5.28626147  3.67665881  2.49114209 ... -6.61787458  1.46460581\n","  2.58161065] (12665,)\n","y hat: [0.99496484 0.97531727 0.92351851 ... 0.00133448 0.81223611 0.92966865]\n","Loss at step 973: -3.348714060994078\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 973: 0.9974733517568101\n","Performing step 974 of gradient descent.\n","z [ 5.28763035  3.67749474  2.49188613 ... -6.61978031  1.46514504\n","  2.58243978] (12665,)\n","y hat: [0.99497169 0.97533738 0.92357105 ... 0.00133195 0.81231833 0.92972285]\n","Loss at step 974: -3.346405503836454\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 974: 0.9974733517568101\n","Performing step 975 of gradient descent.\n","z [ 5.28899789  3.67832978  2.49262952 ... -6.62168426  1.46568389\n","  2.58326825] (12665,)\n","y hat: [0.99497853 0.97535746 0.9236235  ... 0.00132942 0.81240047 0.92977696]\n","Loss at step 975: -3.344101266929316\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 975: 0.9974733517568101\n","Performing step 976 of gradient descent.\n","z [ 5.29036409  3.67916393  2.49337225 ... -6.62358644  1.46622234\n","  2.58409605] (12665,)\n","y hat: [0.99498535 0.9753775  0.92367588 ... 0.00132689 0.81248252 0.92983099]\n","Loss at step 976: -3.3418013376555424\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 976: 0.9974733517568101\n","Performing step 977 of gradient descent.\n","z [ 5.29172896  3.67999719  2.49411432 ... -6.62548684  1.4667604\n","  2.58492319] (12665,)\n","y hat: [0.99499215 0.9753975  0.92372818 ... 0.00132438 0.81256448 0.92988494]\n","Loss at step 977: -3.3395057034477857\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 977: 0.9974733517568101\n","Performing step 978 of gradient descent.\n","z [ 5.29309249  3.68082956  2.49485574 ... -6.62738546  1.46729807\n","  2.58574967] (12665,)\n","y hat: [0.99499894 0.97541747 0.9237804  ... 0.00132187 0.81264636 0.9299388 ]\n","Loss at step 978: -3.337214351788305\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 978: 0.9974733517568101\n","Performing step 979 of gradient descent.\n","z [ 5.2944547   3.68166105  2.49559651 ... -6.62928232  1.46783535\n","  2.58657548] (12665,)\n","y hat: [0.99500572 0.9754374  0.92383254 ... 0.00131937 0.81272815 0.92999259]\n","Loss at step 979: -3.3349272702087873\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 979: 0.9974733517568101\n","Performing step 980 of gradient descent.\n","z [ 5.29581559  3.68249166  2.49633662 ... -6.63117741  1.46837224\n","  2.58740064] (12665,)\n","y hat: [0.99501248 0.97545729 0.9238846  ... 0.00131687 0.81280985 0.93004629]\n","Loss at step 980: -3.3326444462900553\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 980: 0.9974733517568101\n","Performing step 981 of gradient descent.\n","z [ 5.29717515  3.68332139  2.49707609 ... -6.63307075  1.46890874\n","  2.58822514] (12665,)\n","y hat: [0.99501922 0.97547715 0.92393659 ... 0.00131439 0.81289146 0.93009991]\n","Loss at step 981: -3.3303658676617265\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 981: 0.9974733517568101\n","Performing step 982 of gradient descent.\n","z [ 5.29853339  3.68415024  2.49781491 ... -6.63496232  1.46944486\n","  2.58904898] (12665,)\n","y hat: [0.99502594 0.97549697 0.9239885  ... 0.0013119  0.81297299 0.93015346]\n","Loss at step 982: -3.3280915220021035\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 982: 0.9974733517568101\n","Performing step 983 of gradient descent.\n","z [ 5.29989032  3.68497821  2.49855308 ... -6.63685214  1.46998059\n","  2.58987217] (12665,)\n","y hat: [0.99503266 0.97551675 0.92404032 ... 0.00130943 0.81305444 0.93020692]\n","Loss at step 983: -3.3258213970379265\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 983: 0.9974733517568101\n","Performing step 984 of gradient descent.\n","z [ 5.30124593  3.68580531  2.49929061 ... -6.6387402   1.47051593\n","  2.5906947 ] (12665,)\n","y hat: [0.99503935 0.9755365  0.92409207 ... 0.00130696 0.81313579 0.9302603 ]\n","Loss at step 984: -3.3235554805440053\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 984: 0.9974733517568101\n","Performing step 985 of gradient descent.\n","z [ 5.30260022  3.68663153  2.50002749 ... -6.64062652  1.47105089\n","  2.59151658] (12665,)\n","y hat: [0.99504603 0.97555621 0.92414375 ... 0.0013045  0.81321706 0.9303136 ]\n","Loss at step 985: -3.32129376034318\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 985: 0.9974733517568101\n","Performing step 986 of gradient descent.\n","z [ 5.30395321  3.68745689  2.50076373 ... -6.6425111   1.47158546\n","  2.5923378 ] (12665,)\n","y hat: [0.9950527  0.97557588 0.92419534 ... 0.00130205 0.81329825 0.93036682]\n","Loss at step 986: -3.319036224305915\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 986: 0.9974733517568101\n","Performing step 987 of gradient descent.\n","z [ 5.30530489  3.68828137  2.50149933 ... -6.64439393  1.47211965\n","  2.59315838] (12665,)\n","y hat: [0.99505935 0.97559552 0.92424686 ... 0.00129961 0.81337935 0.93041996]\n","Loss at step 987: -3.316782860350034\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 987: 0.9974733517568101\n","Performing step 988 of gradient descent.\n","z [ 5.30665527  3.68910499  2.50223428 ... -6.64627502  1.47265346\n","  2.59397831] (12665,)\n","y hat: [0.99506598 0.97561512 0.9242983  ... 0.00129717 0.81346036 0.93047303]\n","Loss at step 988: -3.314533656440735\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 988: 0.9974733517568101\n","Performing step 989 of gradient descent.\n","z [ 5.30800434  3.68992774  2.5029686  ... -6.64815438  1.47318688\n","  2.59479758] (12665,)\n","y hat: [0.9950726  0.97563469 0.92434967 ... 0.00129473 0.81354129 0.93052601]\n","Loss at step 989: -3.3122886005901533\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 989: 0.9974733517568101\n","Performing step 990 of gradient descent.\n","z [ 5.30935212  3.69074963  2.50370228 ... -6.650032    1.47371993\n","  2.59561621] (12665,)\n","y hat: [0.9950792  0.97565422 0.92440096 ... 0.00129231 0.81362214 0.93057891]\n","Loss at step 990: -3.310047680857071\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 990: 0.9974733517568101\n","Performing step 991 of gradient descent.\n","z [ 5.3106986   3.69157065  2.50443532 ... -6.6519079   1.47425259\n","  2.5964342 ] (12665,)\n","y hat: [0.99508579 0.97567371 0.92445217 ... 0.00128989 0.8137029  0.93063174]\n","Loss at step 991: -3.307810885346927\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 991: 0.9974733517568101\n","Performing step 992 of gradient descent.\n","z [ 5.31204378  3.69239082  2.50516772 ... -6.65378207  1.47478487\n","  2.59725154] (12665,)\n","y hat: [0.99509237 0.97569317 0.9245033  ... 0.00128748 0.81378357 0.93068448]\n","Loss at step 992: -3.305578202211359\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 992: 0.9974733517568101\n","Performing step 993 of gradient descent.\n","z [ 5.31338768  3.69321012  2.50589949 ... -6.65565452  1.47531677\n","  2.59806824] (12665,)\n","y hat: [0.99509893 0.97571259 0.92455436 ... 0.00128507 0.81386416 0.93073715]\n","Loss at step 993: -3.3033496196481704\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 993: 0.9974733517568101\n","Performing step 994 of gradient descent.\n","z [ 5.31473028  3.69402857  2.50663063 ... -6.65752525  1.4758483\n","  2.59888429] (12665,)\n","y hat: [0.99510547 0.97573198 0.92460535 ... 0.00128267 0.81394467 0.93078974]\n","Loss at step 994: -3.3011251259009335\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 994: 0.9974733517568101\n","Performing step 995 of gradient descent.\n","z [ 5.3160716   3.69484617  2.50736114 ... -6.65939427  1.47637945\n","  2.5996997 ] (12665,)\n","y hat: [0.995112   0.97575133 0.92465625 ... 0.00128028 0.81402509 0.93084225]\n","Loss at step 995: -3.2989047092587764\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 995: 0.9974733517568101\n","Performing step 996 of gradient descent.\n","z [ 5.31741163  3.69566291  2.50809101 ... -6.66126157  1.47691022\n","  2.60051448] (12665,)\n","y hat: [0.99511851 0.97577065 0.92470709 ... 0.0012779  0.81410543 0.93089468]\n","Loss at step 996: -3.296688358056415\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 996: 0.9974733517568101\n","Performing step 997 of gradient descent.\n","z [ 5.31875039  3.69647879  2.50882025 ... -6.66312716  1.47744061\n","  2.60132861] (12665,)\n","y hat: [0.99512501 0.97578993 0.92475784 ... 0.00127552 0.81418569 0.93094704]\n","Loss at step 997: -3.294476060673509\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 997: 0.9974733517568101\n","Performing step 998 of gradient descent.\n","z [ 5.32008786  3.69729383  2.50954887 ... -6.66499105  1.47797063\n","  2.60214211] (12665,)\n","y hat: [0.99513149 0.97580918 0.92480853 ... 0.00127315 0.81426586 0.93099931]\n","Loss at step 998: -3.2922678055348498\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 998: 0.9974733517568101\n","Performing step 999 of gradient descent.\n","z [ 5.32142406  3.69810802  2.51027686 ... -6.66685323  1.47850027\n","  2.60295497] (12665,)\n","y hat: [0.99513796 0.97582839 0.92485913 ... 0.00127078 0.81434595 0.93105151]\n","Loss at step 999: -3.2900635811098895\n","(12665, 785)\n","(12665,)\n","(785,)\n","(12665,)\n","Accuracy at step 999: 0.9974733517568101\n"]}]},{"cell_type":"markdown","metadata":{"id":"Dt_XyizgcD7H"},"source":["Evaluate model on test set"]},{"cell_type":"code","metadata":{"id":"RIysfpyCcIN2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cba256d-c64b-4dc7-dd50-8822390dbc33","executionInfo":{"status":"ok","timestamp":1682003082954,"user_tz":-120,"elapsed":199,"user":{"displayName":"Lukas W","userId":"03408245159808875042"}}},"source":["print(\"_______________________________\")\n","print(\"Starting evaluation of test set\")\n","\n","X,y = data_preprocess(x_test, y_test)\n","z = np.dot(X,w) \n","y_hat = sigmoid(z)\n","predictions = (y_hat>0.5).astype(np.int32)\n","accuracy = np.mean(predictions==y)\n","print(\"Accuracy of test set: \" + str(accuracy))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["_______________________________\n","Starting evaluation of test set\n","10000\n","length  2115\n","(2115, 785)\n","(2115, 785)\n","Accuracy of test set: 0.9990543735224586\n"]}]}]}